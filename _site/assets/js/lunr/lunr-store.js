var store = [{
        "title": "[Alexnet] ImageNet Classification with Deep Convolutional Neural Networks",
        "excerpt":"Alexnet Architecture   Alexnet은 8개의 학습가능한 레이어로 구성되어 있다. 구체적으로 5개의 컨볼루션 레이어와 3개의 FC 레이어로 구성되어 있다. 아래 그림에서 보여지는 것과 같이 첫 번째와 두 번째 그리고 두 번째와 세 번째 컨볼루션 레이어 사이에만 Max-Pooling 과 Local Response Normalization 가 있다.       본 논문에서는 입력 되는 이미지는 224 x 224 x 3 형태이지만, 이는 잘못 표기된 것이므로 227 (Height) x 227 (Width) x 3 (Channel) 로 수정되어야 한다.   첫 번째 레이어 (컨볼루션 레이어)           주어진 입력 데이터 (227 x 227 x 3) 에 대해 96개의 11 x 11 x 3 필터 (Filter) 로 컨볼루션 연산을 수행한다. 이 레이어에서는 Stride 을 4로 설정하고, Padding 은 사용하지 않았다.            입력 데이터에 대한 Filter의 크기, Stride의 크기, Padding 여부에 따라서 Feature Map 크기가 결정된다. 컨볼루션 레이어의 출력 데이터의 크기를 산정할 때 사용하는 공식은 다음과 같다 (IH = Image Height, IW = Image  Width, FH = Feature Height, FW = Feature Width)   \\[\\bigg( \\frac{IH - FH + 2 \\cdot Padding}{Stride} + 1, \\frac{IW - FW + 2 \\cdot Padding}{Stride} + 1\\bigg)\\]  \\[\\bigg( \\frac{227 - 11 + 2 \\cdot 0}{4}, \\frac{227- 11 + 2 \\cdot 0}{4} \\bigg) = (55,55)\\]           컨볼루션 연산이 끝난 후의 출력 데이터의 크기 - 55 x 55 x 96            활성화 함수 - ReLU &amp; Normalization            이어서 3 x 3 Max-Pooling (Stride = 2) 레이어에 들어가게 되면 Feature Map이 27 x 27 x 96 으로 변한다. Max-Pooling 레이어의 출력 데이터는 다음과 같은 공식을 사용한다.   \\[\\bigg( \\bigg \\lfloor \\frac{IH - FH}{Stride} + 1 \\bigg \\rfloor, \\bigg\\lfloor\\frac{IW - FW}{Stride} + 1\\bigg\\rfloor \\bigg)\\]  \\[\\bigg( \\frac{55 - 3}{2} + 1,  \\frac{55 - 3}{2} + 1 \\bigg) = (27, 27)\\]      두 번째 레이어 (컨볼루션 레이어)           첫 번째 레이어 출력 데이터 (27 x 27 x 96) 를 두 번째 레이어에서의 입력 데이터로 사용하고, 256 개의 5 x 5 x 96 필터를 사용해 컨볼루션 연산을 한다. 이 레이어에서의 Stride 는 1이고 Padding 은 이전 단계와 다르게 2로 설정되어 있다.            Convolution 연산이 끝난 후의 출력 데이터 크기 - 27 x 27 x 256            활성화 함수 - ReLU &amp; Normalization            첫 번째 컨볼루션 레이어와 비슷하게, 두 번째 컨볼루션 레이어의 출력값이 3 x 3 Maxpooling (Stride = 2) 레이어에 들어가면 Feature Map의 크기는 13 x 13 x 256 으로 변한다.       세 번째 레이어 (컨볼루션 레이어)           384 개의 3 x 3 x 256 필터를 사용해 이전 단계에서의 Feature Map에 Convolution 연산을 해준다.            Stride 과 Padding 둘 다 1 로 설정해주면, 출력 데이터의 크기는 13 x 13 x 384 가 됩니다.            활성화 함수 - ReLU       네 번째 레이어 (컨볼루션 레이어)      384 개의 3 x 3 x 284 필터를 사용해 이전 단계에서의 Feature Map에 Convolution 연산을 해준다.   Stride 과 Padding 둘 다 1 로 설정해주면, 출력 데이터의 크기는 13 x 13 x 384 가 됩니다.   활성화 함수 - ReLU   다섯 번째 레이어 (컨볼루션 레이어)      384 개의 3 x 3 x 284 필터를 사용해 이전 단계에서의 Feature Map에 Convolution 연산을 해준다.   Stride 과 Padding 둘 다 1 로 설정해주면, 출력 데이터의 크기는 13 x 13 x 384 가 됩니다.   활성화 함수 - ReLU   세 번째와 네 번째 레이어와 다르게 3 x 3 Maxpooling (Stride = 2) 을 해주면 6 x 6 x 256 특성맵이 나온다.   여섯 번째 레이어 (FC 레이어)      장난감 블록으로 만든 3D 박스 Feature Map을 상상해보자. 장난감 블록을 한 줄로 높이 쌓아 올린다고 생각하면 된다. 따라서 6 x 6 x 256  Feature Map 한 줄로 쌓아 올리게 되면 줄 (vector) 의 길이 (size)는 9216 가 된다.   9216 차원의 벡터를 4096 차원의 벡터와 연결시켜준다.   활성화 함수 - ReLU   일곱 번째 레이어 (FC 레이어)      이 레이어에서는 4096 차원의 벡터를 똑같은 차원의 벡터와 연결시켜준다.   활성화 함수 - ReLU   여덟 번째 레이어 (FC 레이어)      이 레이어는 Alexnet 구조의 마지막 레이어이고, 4096 차원의 벡터를 1000 차원 (레이블 데이터 종류) 벡터에 연결시켜준다.   활성화 함수 - Softmax   Alexnet Tensorflow 실습   Configuration      SGD (Stochastic Gradient Descent)   Batch Size: 128   Momentum: 0.9   Learning Rate: 0.01   Weight Decay: 0.0005   import tensorflow as tf from tensorflow.keras.layers import Conv2D, Dense, Flatten from tensorflow.keras.layers import  BatchNormalization, Dropout, MaxPool2D from tensorflow.keras.models import Sequential from sklearn.model_selection import train_test_split  # 텐서플로우에서 CIFAR-10 데이터셋 로드 학습데이터 50000, 테스트 데이터 10000 (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()  # CIFAR10 레이블 데이터 CLASS_NAMES= ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']  # 학습데이터에서 train/validation 나누기 train_images, validation_images, train_labels, validation_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)  # 데이터 준비 train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels)) test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)) validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))  IMG_ROWS = 227 IMG_COLS = 227  AUTOTUNE = tf.data.experimental.AUTOTUNE def augment(image,label):     image = tf.image.resize(image, (IMG_ROWS,IMG_COLS)) # CIFAR10 32x32x3 --&gt; 227x227x3      image = tf.image.convert_image_dtype(image, tf.float32)     return image,label  train_ds = (train_ds                 .map(augment)                 .batch(batch_size=32, drop_remainder=True)                 .prefetch(AUTOTUNE)) test_ds=(test_ds                 .map(augment)                 .batch(batch_size=32,drop_remainder=True)) validation_ds = (validation_ds                 .map(augment)                 .batch(batch_size=32, drop_remainder=True))  # 텐서플로우 Alexnet 모델 model = Sequential([     Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(IMG_ROWS,IMG_COLS,3)),     BatchNormalization(),     MaxPool2D(pool_size=(3,3), strides=(2,2)),     Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),     BatchNormalization(),     MaxPool2D(pool_size=(3,3), strides=(2,2)),     Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),     BatchNormalization(),     Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),     BatchNormalization(),     Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),     BatchNormalization(),     MaxPool2D(pool_size=(3,3), strides=(2,2)),     Flatten(),     Dense(4096, activation='relu'),     Dropout(0.5),     Dense(4096, activation='relu'),     Dropout(0.5),     Dense(10, activation='softmax') ])  reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, min_lr=0.00001)  opt = tf.optimizers.SGD(lr=0.01, momentum=0.9) model.compile(loss='sparse_categorical_crossentropy', optimizer= opt, metrics=['accuracy']) model.summary()  model.fit(train_ds, epochs = 50, validation_data = validation_ds, callbacks=[reduce_lr]) model.evaluate(test_ds)   Reference:      https://wikidocs.net/165426   https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html   https://cs231n.github.io/convolutional-networks/  ","categories": ["Deep Learning"],
        "tags": ["Computer Vision"],
        "url": "/deeplearning/alexnet",
        "teaser": null
      },{
        "title": "[Resnet] Deep Residual Learning for Image Recognition",
        "excerpt":"Resnet   VGG 모델이 좋은 성능을 보이면서, 과연 더 깊은 네트워크만으로 성능을 향상시킬 수 있을까? 라는 의문을 가지고 시작된 연구 모델이 Resnet 이다. 아래 사진에서 보이는 것과 같이 CNN 레이어를 20 개에서 56 개로 늘렸을 때 training 과 test error 모두 좋지 않았기 때문이다. 왜 이런 현상이 생기는 것일까? 역전파하다가 가중치에 따른 결과값의 기울기가 0에 가까워지거나 비상적으로 커지기 떄문에 학습이 잘 되지 않을 수 있다. 이 문제를 gradient vanishing/exploding 이라고 한다. 이 외에도 네트워크의 레이어가 어느 정도 깊어지면 성능이 떨어지는 degradation 문제도 있다. 본 논문에서는 이 문제를 residual learning 개념으로 해결하고자 했다.       Residual Block       기존 신경망은 x 값이 입력 데이터로 들어 왔을 때 y 값으로 매핑할 수 있는 함수를 구하는 것이 목표이었다. 하지만 Resnet 모델은 $F(x) + x$ 를 최소화 하는 것을 목표로 한다. 즉, $F(x)$를 0에 가깝게 만들어 $H(x) - x$ (잔차)를 최소화하면서 $H(x) = x$ 가 되도록 학습하는 방법이다. 여기서 입력 데이터인 $x$ 를 사용하기 위해 쓰는 것인 Skip Connection 이다. Skip Connection은 입력 값이 층들 사이를 건너뛰어 출력에 더할 수 있게 하는 역할을 한다.   Residual Architecture           Mini Resnet Tensorflow 실습   import tensorflow as tf from tensorflow.keras.datasets import mnist from tensorflow.keras.layers import Input, Dense, concatenate, Conv2D from tensorflow.keras.layers import BatchNormalization, Flatten, Add, Activation from tensorflow.keras.models import Model  # Identity Block class IdentityBlock(Model):     def __init__(self, filters, kernel_size):         super(IdentityBlock, self).__init__()         self.conv1 = Conv2D(filters, kernel_size, padding='same')         self.bn1 = BatchNormalization()         self.conv2 = Conv2D(filters, kernel_size, padding='same')         self.bn2 = BatchNormalization()         self.relu = Activation('relu')         self.add = Add()          def call(self, inputs):         x = self.conv1(inputs)           x = self.bn1(x)         x = self.relu(x)                    x = self.conv2(x)           x = self.bn2(x)                  x = self.add([x, inputs])         x = self.relu(x)         return x  class ResNet(Model):     def __init__(self, num_classes):         super(ResNet, self).__init__()         self.conv = tf.keras.layers.Conv2D(64, 7, padding='same')         self.bn = tf.keras.layers.BatchNormalization()         self.relu = tf.keras.layers.Activation('relu')         self.max_pool = tf.keras.layers.MaxPool2D((3, 3))         self.id1a = IdentityBlock(64, 3)         self.id1b = IdentityBlock(64, 3)         self.global_pool = tf.keras.layers.GlobalAveragePooling2D()         self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')          def call(self, inputs):         x = self.conv(inputs)         x = self.bn(x)         x = self.relu(x)         x = self.max_pool(x)          x = self.id1a(x)         x = self.id1b(x)          x = self.global_pool(x)         return self.classifier(x)      (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train / 255. x_test = x_test / 255. x_train = x_train[:,:,:,tf.newaxis] x_test = x_test[:,:,:,tf.newaxis]  resnet = ResNet(10) resnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) resnet.fit(x_train, y_train, epochs=5) resnet.evaluate(x_test, y_test)    Reference:      https://arxiv.org/abs/1512.03385   https://wikidocs.net/137252   https://cs231n.github.io/convolutional-networks/   https://junstar92.tistory.com/146  ","categories": ["Deep Learning"],
        "tags": ["Computer Vision"],
        "url": "/deeplearning/resnet",
        "teaser": null
      },{
        "title": "[VGG] Very Deep Convolutional Networks For Large-Scale Image Recognition",
        "excerpt":"VGG Architecture     VGG 특징      기본적으로 필터 (Filter) 사이즈는 3 x 3 으로 설정했고, VGG-16 이후 모델은 필터 1 x 1 도 함께 사용했다.   필터 3 x 3 를 사용하면, 더 큰 사이즈의 필터를 사용했을 때 보다 (ex. 7 x 7) 깊은 네트워크 층을 쌓을 수 있다는 것과 파라미터의 개수를 줄일 수 있다는 장점이 있다.            두 개의 3 x 3 필터를 사용한다는 것은 한 개의 5 x 5 필터를 사용하는 것과 동일한 Effective Receptive Field를 가지고 있다. 따라서 더 깊이 쌓으면서 모델의 비선형성을 증가시킬 수 있다.       3 개의 3 x 3 필터의 파라미터 개수 3 * (3 * 3 * $Channel_{prev}$  + 1) * $Channel_{curr}$ &lt; 1 개의 7 x 7 필터의 파라미터 개수 1 * (7 * 7 * $Channel_{prev}$ + 1) * $Channel_{curr}$           필터 1 x 1 의 사용 이점: 차원 축소, 비선형성의 증가, 그리고 계산량 감소            예를 들어, 12 x 12 x 3 입력 데이터에 대해 5개의 3 x 3 필터를 적용하면 최종 파라미터의 계산량은 12 * 12 * 3 * 3 * 3 * 5 = 19440 번이지만, 1 개의 1 x 1 적용하고 5개의 3 x 3 필터를 적용하면  12 * 12 * 3  + 12 * 12 * 1 * 5 * 3 * 3 = 6912 번의 계산량으로 줄어든다.           VGG-16 Tensorflow 실습   Configuration           Mini-batch Gradient Descent            Batch Size: 256            Momentum: 0.9            Learning Rate: 0.01            Weight Decay:  $L_{2}$ multiplier set to $5 · 10^{−4}$       import tensorflow as tf from tensorflow.keras.layers import Input, Dense, Conv2D from tensorflow.keras.layers import Flatten, MaxPooling2D from tensorflow.keras.optimizers import Adam, RMSprop from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping from tensorflow.keras.models import Model, Sequential  from tensorflow.keras.datasets import cifar10 from sklearn.model_selection import train_test_split  import tensorflow_addons as tfa  (train_images, train_labels), (test_images, test_labels) = cifar10.load_data() train_images, validation_images, train_labels, validation_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)  # 데이터 준비 train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels)) test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)) validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))  IMG_ROWS = 224 IMG_COLS = 224  AUTOTUNE = tf.data.experimental.AUTOTUNE def augment(image,label):     image = tf.image.resize(image, (IMG_ROWS,IMG_COLS)) # CIFAR10 32x32x3 --&gt; 227x227x3      image = tf.image.convert_image_dtype(image, tf.float32)     return image,label  train_ds = (train_ds                 .map(augment)                 .batch(batch_size=32, drop_remainder=True)                 .prefetch(AUTOTUNE)) test_ds=(test_ds                 .map(augment)                 .batch(batch_size=32,drop_remainder=True)) validation_ds = (validation_ds                 .map(augment)                 .batch(batch_size=32, drop_remainder=True))   model = Sequential() model.add(Conv2D(input_shape = (IMG_ROWS, IMG_COLS, 3), filters = 64, kernel_size = (3,3), padding = \"same\", activation = \"relu\")) model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = \"same\", activation = \"relu\")) model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))  model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = \"relu\")) model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = \"relu\")) model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))  model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\")) model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\")) model.add(Conv2D(filters = 256, kernel_size = (1,1), padding = \"same\", activation = \"relu\")) model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))  model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\")) model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\")) model.add(Conv2D(filters = 512, kernel_size = (1,1), padding = \"same\", activation = \"relu\")) model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))  model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\")) model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\")) model.add(Conv2D(filters = 512, kernel_size = (1,1), padding = \"same\", activation = \"relu\")) model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))  model.add(Flatten()) model.add(Dense(units = 4096, activation = \"relu\")) model.add(Dense(units = 4096, activation = \"relu\")) model.add(Dense(units = 10, activation = \"softmax\"))  opt = tfa.optimizers.SGDW(weight_decay=0.0005, learning_rate=0.01, momentum=0.9) model.compile(optimizer = opt, loss= 'sparse_categorical_crossentropy', metrics = ['accuracy']) reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, min_lr=0.00001) model.fit(train_ds,             epochs = 50,             validation_data = validation_ds,             callbacks = [reduce_lr]) model.evaluate(test_ds)   Reference:      https://wikidocs.net/165427   Stanford CS231n   https://ai.plainenglish.io/vggnet-with-tensorflow-transfer-learning-with-vgg16-included-7e5f6fa9479a   https://arxiv.org/abs/1409.1556  ","categories": ["Deep Learning"],
        "tags": ["Computer Vision"],
        "url": "/deeplearning/vgg",
        "teaser": null
      },{
        "title": "[GoogLeNet] Going Deeper with Convolutions",
        "excerpt":"GoogLeNet   CNN 모델 성능 향상을 위해 가장 직접적인 방법이 네트워크의 깊이를 늘리는 방법이다. 하지만 네트워크가 깊어지면 파라미터 수와 연산량이 많아지고 overfitting 문제에 노출될 수 있다. 따라서, 이러한 문제를 해결하기 위해 Szegedt et al. 은 네트워크의 구조적 변화가 필요하다고 생각했고, Inception 모듈로 구성된 GoogLeNet으로 문제를 해결했다.   GoogLeNet Architecture       GoogLeNet은 22 레이어 층으로 구성되어 있고, 총 3개의 특징이 있다      1 x 1 Filter Conv   Inception Module   Auxiliary Classifier   1 x 1 Filter Conv   1 x 1 필터는 차원 축소, 비선형성의 증가, 그리고 계산량 감소를 시킬 수 있다는 장점이 있다. GoogLeNet에서 1 x 1 필터를 Feature Map 차원의 개수를 줄이는 목적으로 사용한다. 예를 들어, 11 x 11 x 300 Feature Map이 있다고 가정해보자. Feature Map에 30개의 5 x 5 필터로 Convolution을 적용하면 11 x 11 x 30 Feature Map이 생성된다 (Stride = 1, Padding = 0). 이때 필요한 연산량은 11 x 11 x 30 x 5 x 5 x 300 = 27225000 이 된다.   이번에는 11 x 11 x 300 Feature Map에 먼저 5개의 1 x 1 필터를 적용한 뒤, 30개의 5 x 5 필터를 적용해보자. 그러면  11 x 11 x 5 Feature Map이 생성되고, 이 Feature Map에 5 x 5 필터를 적용해주면 사이즈가 11 x 11 x 30인 Feature Map이 생성된다. 그러면 연산량은 얼마일까? 1 x 1 필터를 적용했을 때의 연산량은 11 * 11 * 300 * 1 * 1 * 5 = 181500이고, 5 x 5 필터를 적용했을 때의 연산량은 11 * 11 * 5 * 5 * 5 * 30 = 21780 이다. 따라서 총 연산량은 181500 + 21780 = 203280 이다. 1 x 1 필터를 중간에 사용했을 때  더 적은 연산량을 가짐을 확인할 수 있다. 연산량을 줄일 수 있다는 점은 네트워크를 더 깊이 만들 수 있게 도와준다는 점에서 중요하다.   Inception Module     GoogLeNet은 총 9개의 Inception Module 을 사용하고 있다. GoogLeNet에서 사용된 모듈은 1x1 Conv가 포함된 (b) 모델이다. 위에서 설명했듯이 1x1 필터로 Conv 하면 Feature Map 의 차원의 개수를 줄여줄 수 있다. 1x1 필터를 제외한 버전을 살펴보면, 이전 층에서 생성된 Feature Map을 1x1 필터, 3x3 필터, 5x5 필터, 3x3 Max Pooling 결과 얻은 Feature Map들을 모두 함께 쌓아준다.   Auxiliary Classifier   신경망의 깊이가 깊어질 수록 vanishing gradient 문제가 발생한다. Vanishing Gradient 문제란 back propagation 하는 과정에서 가중치를 업데이터하면서 gradient가 점점 작아져서 0이 되어버린 것을 의미한다. 따라서 신경망이 제대로 학습되지 않을 수 있다.   Auxiliary classifier는 학습시에만 사용하고 inference 할 때 GoogLeNet 중간에 있는 2개의 auxiliary classifier를 모두 제거해야 한다.   GooLeNet Tensorflow 실습   import tensorflow as tf from tensorflow.keras.datasets import mnist from tensorflow.keras import layers, Sequential, Model  class Inception(layers.Layer):     def __init__(self, n1x1, n3x3_reduce, n3x3, n5x5_reduce, n5x5, pool_proj):         super(Inception, self).__init__()          self.b1 = Sequential([             layers.Conv2D(n1x1, (1, 1)),             layers.BatchNormalization(),             layers.ReLU()         ])         self.b2 = Sequential([             layers.Conv2D(n3x3_reduce, (1, 1)),             layers.BatchNormalization(),             layers.ReLU(),             layers.Conv2D(n3x3, (3, 3), padding='same'),             layers.BatchNormalization(),             layers.ReLU()         ])         self.b3 = Sequential([             layers.Conv2D(n5x5_reduce, (1, 1)),             layers.BatchNormalization(),             layers.ReLU(),             layers.Conv2D(n5x5, (3, 3), padding='same'),             layers.BatchNormalization(),             layers.ReLU(),             layers.Conv2D(n5x5, (3, 3), padding='same'),             layers.BatchNormalization(),             layers.ReLU(),         ])         self.b4 = Sequential([             layers.MaxPool2D((3, 3), 1, padding='same'),             layers.Conv2D(pool_proj, (1, 1)),             layers.BatchNormalization(),             layers.ReLU(),         ])      def call(self, x):         x = tf.concat([self.b1(x), self.b2(x), self.b3(x), self.b4(x)], axis=3)         return x   class GoogleNet(Model):     def __init__(self, num_classes, input_shape=(28, 28, 1)):         super(GoogleNet, self).__init__()         self.layer1 = Sequential([             layers.Input(input_shape),             layers.Conv2D(192, (3, 3), padding='same'),             layers.BatchNormalization(),             layers.ReLU()         ])         self.layer2 = Sequential([             Inception(64, 96, 128, 16, 32, 32),             Inception(128, 128, 192, 32, 96, 64),             layers.MaxPool2D((3, 3), 2, padding='same'),         ])         self.layer3 = Sequential([             Inception(192, 96, 208, 16, 48, 64),             Inception(160, 112, 224, 24, 64, 64),             Inception(128, 128, 256, 24, 64, 64),             Inception(112, 144, 288, 32, 64, 64),             Inception(256, 160, 320, 32, 128, 128),             layers.MaxPool2D((3, 3), 2, padding='same'),         ])         self.layer4 = Sequential([             Inception(256, 160, 320, 32, 128, 128),             Inception(384, 192, 384, 48, 128, 128)         ])         self.layer5 = Sequential([             layers.GlobalAveragePooling2D(),             layers.Dropout(0.4),         ])         self.fc = layers.Dense(num_classes, activation='softmax')      def call(self, inputs, training=False):         x = self.layer1(inputs, training=training)         x = self.layer2(x, training=training)         x = self.layer3(x, training=training)         x = self.layer4(x, training=training)         x = self.layer5(x, training=training)         x = tf.reshape(x, (x.shape[0], -1))         x = self.fc(x)         return x      (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train / 255. x_test = x_test / 255. x_train = x_train[:,:,:,tf.newaxis] x_test = x_test[:,:,:,tf.newaxis] # print(x_train.shape) model = GoogleNet(10) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) model.fit(x_train, y_train, epochs=5) model.evaluate(x_test, y_test)    Reference:      https://github.com/marload/ConvNets-TensorFlow2/blob/master/models/GoogLeNet.py   https://bskyvision.com/   https://arxiv.org/abs/1409.4842  ","categories": ["Deep Learning"],
        "tags": ["Computer Vision"],
        "url": "/deeplearning/googlenet",
        "teaser": null
      },{
        "title": "[Seq2Seq] Sequence to Sequence Learning with Neural Networks",
        "excerpt":"Introduction   딥러닝 기술이 음성인식, 기계번역 등의 문제를 풀 수 있는 실마리를 제공하고 있지만, 입력 데이터와 출력 데이터의 길이가 고정되어야 한다는 한계가 있다. 본 논문에서는 LSTM 기반의 Encoder-Decoder 모델에 고정된 길이의 입력 데이터를 넣어주고, 가변 길이의 출력 데이터를 생성해 번역 문제를 풀고자 했다.     Seq2Seq   LSTM의 목표는 입력 데이터에 대한 출력 데이터에 대한 조건부 확률을 아래 식과 같이 평가하는 것이다. LSTM은 우선 입력 데이터에 대하여 고정된 차원의 context vector를 연산하고, 아래 수식으로 연산되는 확률을 계산한다. 이때 $p(y_{t}|v,y_1,y_2,…,y_{t-1})$  분포는 단어에 대한 softmax에 해당된다.   \\[p(y_1,...,y_{T^{'}}| x_{1}, ..., x_{T}) = \\prod p(y_t|v, y_{1}, ...,y_{t-1})\\]    앞서 언급했다시피 Seq2Seq은 입력 데이터를 위한 Encoder와 출력 데이터를 위한 Decoder를 사용한다. Encoder는 입력 데이터의 단어들을 순차적으로 입력받아서 context vector 만든다. 입력 데이터의 정보가 context vector로 모두 압축되면 Encoder는 context vector를 Decoder로 넣어주고, Decoder는 번역된 단어를 한 개씩 순차적으로 출력하게 된다.   본 논문에서는 입력 데이터를 역순으로 배치한 후 모델에 넣어줬다. 입력 문장 데이터의 시작점과 출력 문장 데이터의 시작점이 멀기 때문에 입력 데이터를 거꾸로 넣어줌으로써 minimal time lag 효과를 절감했다.   Experiment           데이터셋: WMT 14 English to French 데이터중 일부 선택해 사용했으며, 일반적인 neural language model은 각 단어의 벡터 표현에 의존적이기 때문에 두 언어에 대해 고정된 크기의 단어 사전을 사용했다. 단어 사전에 없는 모든 out-of-vocabulary word는 UNK 토큰으로 처리했다.            모델 가중치: -0.08 ~ 0.08 uniform distribution            SGD without momentum (learning rate=0.7, epoch 5 이후 매 epoch 마다 lr 절반으로 만듦)            4개의 LSTM 레이어 사용               Encoder-Decoder Tensorflow 실습   https://wikidocs.net/24996   Reference:      https://arxiv.org/abs/1409.3215   https://wikidocs.net/24996   https://coshin.tistory.com/47   ","categories": ["Deep Learning"],
        "tags": ["Natural Language Processing"],
        "url": "/deeplearning/seq2seq",
        "teaser": null
      },{
        "title": "[NMF] Non-negative Matrix Factorization",
        "excerpt":"Introduction  NMF (Non-negative matrix factorization)는 negative value (데이터)를 포함하지 않은 행렬 $V$를 negative value를 포함하지 않은 행렬 $W$ (가중치 행렬)와 $H$(특성 행렬)의 곱으로 분해하는 알고리즘이고 컴퓨터비전, 추천 시스템 등 다양한 분야에 쓰인다. NFM 알고리즘이 다른 차원 축소 알고리즘과 다른 점은 데이터의 특성인 non-negativity를 보장 받을 수 있다는 것이다.   \\[\\mathbf{V} = W \\times H\\]  NMF 알고리즘 실습   다음은 얼굴 데이터셋에 대해 진행한 NMF (components = 3) 예시이다.  # 참고: https://jhryu1208.github.io/data/2020/12/10/ML_NMF/ import numpy as np from sklearn.decomposition import NMF from sklearn.datasets import fetch_lfw_people from sklearn.model_selection import train_test_split  people = fetch_lfw_people(min_faces_per_person=20, resize=0.7) image_shape = people.images[0].shape  mask = np.zeros(people.target.shape, dtype=np.bool) for target in np.unique(people.target):     mask[np.where(people.target == target)[0][:50]] = 1      X_people = people.data[mask] y_people = people.target[mask]  X_people = X_people / 255.0  X_train, X_test, y_train, y_test = train_test_split(X_people,y_people, stratify=y_people, random_state=0)   nmf = NMF(n_components=3, random_state=0, max_iter=1000, tol=1e-3) nmf.fit(X_train) X_train_nmf = nmf.transform(X_train) X_test_nmf = nmf.transform(X_test)  fig, axes = plt.subplots(3, 5, figsize=(15, 12),                          subplot_kw={'xticks': (), 'yticks': ()}) for i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):     ax.imshow(component.reshape(image_shape))     ax.set_title(\"component {}\".format(i))  ","categories": ["Machine Learning"],
        "tags": ["Recommender System"],
        "url": "/machinelearning/nmf",
        "teaser": null
      },{
        "title": "[Buy It Again] Modeling Repeat Purchase Recommendations",
        "excerpt":"Current Problem   이커머스 기업이 사용하는 추천 기능은 대부분 어떤 상품을 추천할 것인가에만 맞추어져 있다. 이에 반해 고객이 한 번 이상 구매한 상품을 언제 추천해야 하는 것인지에 대한 연구가 활발히 이루어지지 못하고 상태이다. 특히 생활용품과 같이 반복적으로 구매가 일어나는 상품의 경우, 재구매 시기를 적절히 예상하여 상품을 추천해준다면 ‘보다 더 편리한’ ‘더 나은’ 고객 경험을 제공해줄 수 있다.   아마존에서 발표한 Repeat Purchase Recommendations 은 고객의 과거 구매 데이터를 바탕으로 고객이 필요한 상품을 다시 추천해주는 기능이다. 예를 들어 1개월 전 아마존 어플에서 펩시 (210ml x 30캔) 을 구매한 이력이 있는 고객에게 상품을 시기적절하게 추천해주는 기능이다.   Purchase Probability Density (PPD)   PPD 를 다음과 같이 정의할 수 있다 \\(P_{A_i}(t_{k+1}=t| t_{1}, t_{2}, t_{3}, …, t_{k})\\)   고객의 구매이력 정보를 가지고 고객의 제품 재구매 확률을 계산하고 이를 기반으로 상품을 추천까지 해준다. Bhagat et al. 2018 논문에서는 RCP (Repeat Customer Probability), ATD (Aggregate Time Distribution), PG (Poisson-Gamma), MPG (Modified Poisson-Gamma) 총 4개의 모형을 사용해 $P_{A_i} (⋅)$ 구하고자 했다   Assumption1: 서로 다른 제품을 구매할 사건은 독립이다   Assumption2:  \\(P_{A_i} (t_{k+1}=t\\ | \\ t_1, t_2, t_3, …, t_k)≈Q(A_i)\\cdot R_{A_i} (t_{k+1}| t_1, .., t_k)\\)   $Q(A_i)$: 고객이 제품을 구매한 횟수가 k 인 경우 (k+1) 번째로 제품을 구매할 재구매 확률   $R_{A_i}$: 고객이 제품을 재구매할 때의 $t_{k+1}$ 분포   1. Repeat Customer Probability (RCP)   \\[𝑅𝐶𝑃_{𝐴_𝑖}= \\frac{\\# \\ customers \\ who \\ bought \\  product \\ 𝐴_𝑖 \\ more \\ than \\ once}{\\# \\ customers \\ who \\ bought \\  product \\ 𝐴_𝑖 \\ at \\ least \\ once}\\]  제품 $A_i$ 을 구매한 고객의 수를 활용한 베이스라인 확률 모델 (Time-Independent)   Assumption:  \\(P_{A_i} (t_{k+1}=t\\ | \\ t_1, t_2, t_3, …, t_k)≈Q(A_i)≈RCP_{A_i}\\)   여기서 $R_{A_i}(\\cdot)$ 은 고정된 r 값 이라고 가정한다.   임계값을 사용해 추천 모델의 품질 향상: $RCP_{A_i}$ &gt; Threshold   Limitation: 고객이 방금 전에 구매한 제품이 여전히 추천 제품 우선 순위에 올라가 있을 수 있음   2. Aggregate Time Distribution (ATD) 모델   \\[𝑅_{𝐴_𝑖} (𝑡)=ln⁡ℕ(𝑡;\\mu_{i}, \\sigma_{i})=\\frac{1}{\\sqrt{2𝜋}  𝑡\\cdot\\sigma_{i}}⋅exp⁡\\bigg[-\\frac{(lnt - \\mu_i)^2}{2\\sigma_i^2} \\bigg], 𝑡&gt;0\\]  제품 $A_i$를 재구매까지 소요된 기간이 로그 정규 분포를 따른다 것을 실험적으로 확인했다. 로그 정규 분포는 Positively Right Skewed 한 분포이기 때문에 항상 양수값만을 가진다. 따라서 음수값을 가짐으로 인해 발생하는 문제에 대한 대안이 될 수 있다. 예를 들어 상품을 재구매까지의 간격, 첫 구매 소요 시간, 구매 가격대 등은 전부 양수 값만 갖는다.   로그 정규 분포를 사용해 어떤 사건의 발생하는 정도가 급격히 증가했다가 낮아지는 것을 모델화 수 있다. 예를 들어, 물건을 구입하거나 이사를 가거나 하는 사건은 한 번 발생하고 나면 발생 확률이 급격이 낮아지는 사건을 모델화 할 수 있다.   주어진 데이터가 로그 정규 분포를 가정하고 있는지 확인하기 위해 MLE로 모수를 구했다. 로그 정규 분포의 모수 $\\mu_i$ 와 $\\sigma_i$ 를 구하는 방법은 다음과 같다   \\[Likelihood \\ LN \\frac{1}{(2\\pi)^{\\frac{n}{2}} \\cdot (\\sigma_i^2)^{\\frac{n}{2}}} \\cdot \\prod \\bigg(\\frac{1}{t_z} \\bigg) \\cdot exp \\bigg(-\\frac{1}{2\\sigma^2_i} \\sum ln(t_z - \\mu_i)^2 \\bigg)\\]  \\[Likelihood \\ LLN = -\\frac{𝑛}{2}ln⁡(2\\pi)−\\frac{𝑛}{2}ln⁡(\\sigma_i^2)−\\frac{1}{2\\sigma_i^2}\\sum ln(t_z - \\mu_i)^2 - \\sum ln(t_z)\\]  \\[\\hat{\\mu_{mle}} = \\frac{1}{n}\\sum ln(t_z), \\ \\hat{\\sigma_{mle}^2} = \\frac{1}{n} \\sum (lnt - \\mu_i)^2\\]  그리고 논문에서는 데이터를 theoretical 로그정규분포와 비교하기 위해 qqplot을 사용했다.        그래프 왼쪽 아래, 점들이 선 위 - 데이터 분포의 왼쪽 꼬리가 로그정규분포의 것보다 짧다.   그래프 오른쪽 위, 점들이 선 아래 - 데이터 분포의 오른쪽 꼬리가 로그정규분포의 것보다 짧다.   Assumption:  \\(P_{A_i} (t_{k+1}=t\\ | \\ t_1, t_2, t_3, …, t_k)≈R_{A_i}(t)\\)   여기서 $Q_{A_i}(\\cdot)$ 은 고정된 q 값 이라고 가정한다.   임계값을 사용해 추천 모델의 품질 향상: $R_{A_i}(t)$ &gt; Threshold   Limitation: 사람마다 제품을 사용하는 속도는 전부 다 다름. 어떤 사람은 각휴지 화장지 20매를 3주 만에 사용하고 어떤 고객은 동일 제품을 1주일만에 사용할 수 있으므로 사람에 따라 제품 구매 주기가 다를 수 있음.   3. Poisson-Gamma Mixture (PG) 모델   포아송 분포와 감마 분포의 혼합 모형은 음이항 분포 (Negative Binomial Distribution) 라고도 부른다. 두 분포를 함께 사용한다는 건 포아송의 모수가 감마 분포를 따르도록 만드는 것을 의미한다. 포아송 분포의 모수는 일정한 기간 동안의 재구매율이고, 고객마다 재구매율이 다르기 때문에 감마분포를 따른다고 가정한다.   \\[𝑋 \\sim 𝑃𝑜𝑖𝑠𝑠𝑜𝑛(\\lambda_i)\\]  \\[\\lambda_i \\sim 𝐺𝑎𝑚𝑚𝑎(\\alpha, \\beta)\\]  관련 기초 통계 분포 개념정리:   포아송 분포: 단위 시간 동안의 성공 횟수에 대한 분포 (기간을 어떻게 정의하냐에 따라 ‘Rate’로도 표현 가능)   감마 분포: 사건을 n번 시행할 때까지의 총 시간을 분포 (지수 분포는 특수한 케이스)   음이항 분포: 사건이 n번 발생할 때까지의 시행하는 경우의 시행 횟수의 분포 (기하 분포는 특수한 케이스)         4. Modified Poisson-Gamma (MPG) 모델   \\[\\lambda_{A_{i,}C_{j}} = \\frac{k + \\alpha_{A_i}}{t_{purch} + 2 \\cdot |t_{mean} - t| + \\beta_{A_i}}\\]  \\(t_{purch}\\): 첫 구매부터 마지막 구매까지의 기간   \\(t_{mean}\\): 평균 재구매 간격   \\(t\\): 마지막 구매부터 집계일까지의 기간   변형된 포아송-감마 혼합 모형은 하나의 재구매율 \\(\\lambda\\) 모수를 사용하고, \\(\\lambda\\)  는 고객이 가장 최근에 재구매한 제품 \\(A_i\\)의 Time 과 Dependent 하다고 가정한다. 그리고 PG 모형과 동일하게 재구매율은 감마분포를 따른다고도 가정한다. (MPG 모델에서의 \\(\\lambda\\) 는 \\(t&lt;2 ∗ t_{mean}\\) 일 때만 사용한다 )   포아송 모수(\\(\\lambda\\))는 t = 0 to t = \\(t_{mean}\\)  일 때 커지고, t = \\(t_{mean}\\)  to t = 2 ∗ \\(t_{mean}\\) 일때는 작아지는 구조이다. 고객이 평균적으로 제품 \\(A_i\\)  를 재구매 하는 시간 간격과 가까워지면 재구매율이 높아지고, 멀어질 수록 재구매율이 낮아지게 만들어졌다.   \\[P_{A_i} (t_{k+1}=t\\ | \\ t_1, t_2, t_3, …, t_k)≈Q(A_i) \\cdot R_{A_{i,}C_{j}}(t)\\]  where \\(Q(A_i) = 𝑅𝐶𝑃_{𝐴_𝑖}= \\frac{\\# \\ customers \\ who \\ bought \\  product \\ 𝐴_𝑖 \\ more \\ than \\ once}{\\# \\ customers \\ who \\ bought \\  product \\ 𝐴_𝑖 \\ at \\ least \\ once}\\)   \\(RCP_{A_i}\\) 은 time-independent signal 도 추천모델에 활용한다. 특정 상품을 재구매 한다는 것은 신규 고객 혹은 기존 고객을 만족시키고 있다는 의미로 해석할 수 있다. 만약 \\(RCP_{A_i}\\) 계산을 생략하고 고객이 최근에 구매한 제품을 바탕을 추천을 한다면 sub-optimal 한 고객경험을 제공하게 된다.   Offline Experiments 결과        ATD, PG, 그리고 MPG 모델이 RCP 베이스라인 모델보다 성능이 좋음.   PG/MPG 은 총 구매 행동 패턴 외에 고객의 구매 신호를 사용할 수 있어서 RCP/ATD 보다 성능이 좋음.   Time-dependent/independent signal 를 모두 사용한 MPG 모델이 성능이 가장 좋음.   OnlineExperiments 결과        기존 추천모델보다 ATD 모델이 CTR 7.1% 높음   ATD 모델보다 MPG 모델이 CTR 1.3% 높음   오프라인 실험에서의 결과와 일치함 (ATD &lt; MPG 성능)   Reference:      https://assets.amazon.science/40/e5/89556a6341eaa3d7dacc074ff24d/buy-it-again-modeling-repeat-purchase-recommendations.pdf  ","categories": ["Statistics"],
        "tags": ["Marketing Science"],
        "url": "/statistics/buyitagain",
        "teaser": null
      },{
        "title": "[Transformer] Attention Is All You Need",
        "excerpt":"Introduction   RNN, LSTM, 그리고 GRU 활용 모델은 기계번역 등의 문제에서 뛰어난 성과를 보였다. 하지만 RNN 계열 모델은 재귀적인 특성 때문에 병렬 처리 연산이 불가능하다는 것이 치명적인 단점이다. 그러면 RNN의 어떤 점이 재귀적인 것일까? RNN 계열 모델은 이전 단계에서 계산한 $h_{t-1}$ 로 현 단계의 $h_t$ 를 순차적으로 생성하는 부분이 재귀적인 특성을 보여준다. 따라서 RNN 계층의 순환 구조가 연산을 병렬화할 수 없게 만든다. 그리고 RNN 계열 모델은 또 한가지의 문제점이 존재한다. 입력과 출력 간의 대응되는 단어들 사이의 거리가 멀수록 그 관계를 모델이 잘 학습하지 못한다 (Long-term dependency problem). 이러한 단점을 보완하고자 seq2seq 구조에서의 Attention만을 사용하는 것을 바로 Transformer 라고한다.   Transformer Architecture     지금까지의 Transduction (변환) 모델은 대부분 RNN 계열의 encoder-decoder 구조를 가지고 있는 반면, Transformer은 encoder-decoder 구조를 사용하지만 그 내부는 attention 과 point-wise feed forward network 만으로 구성되어 있다.   Encoder-Decoder Stacks   Encoder는 동일한 레이어의 구성으로 6개가 stacked 되어있고, 각 레이어는 아래 2개의 서브레이어로 이루어져 있다.      Multi-head self-attention   position-wise fully connected feed-forward network   각 서브레이어의 출력값은 $LayerNorm(x + SubLayer(x))$ 으로 skip connection 과 normalization 을 적용했다.   Decoder은 6개의 동일한 레이어로 stacked 되어있지만, 3 개의 서브레이어로 이루어져 있다.      Multi-head self-attention   Masked Multi-head self-attention   position-wise fully connected feed-forward network   Encoder와 동일하게 skip connection와 normalization을 사용하고, Decoder가 출력값을 생성할 때 다음 출력(미래)에서 정보를 얻는 것을 방지하기 위해 masking을 사용한다.   Attention &amp; Multi-head Attention     Scaled Dot-Product Attention 의 입력값으로 $d_k$ 차원의 query-key와 $d_v$ 차원의 value가 들어간다. 가장 먼저 query와 key의 dot product 을 계산하고, $\\sqrt{d_k}$로 나눈 값을 softmax 함수를 통해 values 의 가중치 값을 구한다. 여기서 $\\frac{1}{\\sqrt{d_k}}$ 값을 곱해주지 않으면 additive attention 보다 성능이 떨어진다고 한다.   \\[Attention(Q, K, V) = softmax(\\frac{Q \\cdot K^T}{\\sqrt{d_k}})\\cdot V\\]  \\[MultiHead(Q, K, V ) = Concat(head_1, ..., head_h)W^{O}\\]  \\[where \\ \\ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\\]  \\[W_i^Q \\in R^{ d_{model} \\times d_k}, W_i^K \\in R^{d_{model} \\times d_k}, W_i^V \\in R^{d_{model} \\times d_v}\\]  본 논문에서는 8 개의 head (attention)  를 사용했다. 그리고 $d_k$ = $d_v$ = $\\frac{d_{model}}{h}$ = 64 를 사용했다.   Point-wise Feed forward Network   Multi-head self attention 레이어에서 출력된 값을 입력 값으로 받고, ReLU 활성함수를 사용한다. Point-wise Feed Forward Network (FFN) 의 수식은 아래와 같다 (이는 conv 1 x 1 연산을 두 번하는 것도 동일하다).   \\[FFN(x) = max(0, xW_i + b_1)W_2 + b_2\\]  Positional Encoding   Transformer 에서는 RNN 계열의 모델을 사용하지 않기 때문에 sequence에 있는 원소들의 위치에 대한 정보도 함께 넣어줘야 한다. 그래서 Encoder와 Decoder이 시작하기 전에 positional encoding를 입력하고 embedding에 더해준다. 모델에서 위치 정보를 추가하기 위해 사용한 것은 사인과 코사인함수이다.   \\[PE_{(pos,2i)}=sin\\bigg(\\frac{pos}{10000^{\\frac{2i}{dmodel}}}\\bigg)\\]  \\[PE_{(pos,2i + 1)}=cos\\bigg(\\frac{pos}{10000^{\\frac{2i}{dmodel}}}\\bigg)\\]  Results   Machine Translation (EN-DE &amp; EN-FR) 에 대한 실험 결과이다. Transformer (base model)만 봐도 EN-DE 기계번역 문제에서 가장 높은 성능을 보여줬고, 특히 Transformer (big) 은 EN-DE/EN-FR 기계번역 문제에서 모두  state-of-the-art를 수준의 성능을 보여 주었다.     Tensorflow 실습   트랜스포머 챗봇 코드: https://github.com/ukairia777/tensorflow-transformer   import pandas as pd import urllib.request import tensorflow_datasets as tfds import tensorflow as tf import time import numpy as np import matplotlib.pyplot as plt import re  # urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")  train_data = pd.read_csv('./ChatBotData.csv')  # 서브워드텍스트인코더를 사용하여 질문과 답변을 모두 포함한 단어 집합(Vocabulary) 생성 tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(     train_data['Q'] + train_data['A'], target_vocab_size=2**13)  # 시작 토큰과 종료 토큰에 대한 정수 부여. START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]  # 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2 VOCAB_SIZE = tokenizer.vocab_size + 2  print('시작 토큰 번호 :',START_TOKEN) print('종료 토큰 번호 :',END_TOKEN) print('단어 집합의 크기 :',VOCAB_SIZE)  sample_string = train_data['Q'][0] # encode() : 텍스트 시퀀스 --&gt; 정수 시퀀스 tokenized_string = tokenizer.encode(sample_string) print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))  # decode() : 정수 시퀀스 --&gt; 텍스트 시퀀스 original_string = tokenizer.decode(tokenized_string) print ('기존 문장: {}'.format(original_string))  # 최대 길이를 40으로 정의 MAX_LENGTH = 40  # 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩 def tokenize_and_filter(inputs, outputs):     tokenized_inputs, tokenized_outputs = [], []      for (sentence1, sentence2) in zip(inputs, outputs):     # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가         sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN         sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN          tokenized_inputs.append(sentence1)         tokenized_outputs.append(sentence2)      # 패딩     tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(         tokenized_inputs, maxlen=MAX_LENGTH, padding='post')     tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(         tokenized_outputs, maxlen=MAX_LENGTH, padding='post')          return tokenized_inputs, tokenized_outputs  questions, answers = tokenize_and_filter(train_data['Q'], train_data['A']) print('질문 데이터의 크기(shape) :', questions.shape) print('답변 데이터의 크기(shape) :', answers.shape)  # 텐서플로우 dataset을 이용하여 셔플(shuffle)을 수행하되, 배치 크기로 데이터를 묶는다. # 또한 이 과정에서 교사 강요(teacher forcing)을 사용하기 위해서 디코더의 입력과 실제값 시퀀스를 구성한다. BATCH_SIZE = 64 BUFFER_SIZE = 20000  # 디코더의 실제값 시퀀스에서는 시작 토큰을 제거해야 한다. dataset = tf.data.Dataset.from_tensor_slices((     {         'inputs': questions,         'dec_inputs': answers[:, :-1] # 디코더의 입력. 마지막 패딩 토큰이 제거된다.     },     {         'outputs': answers[:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다.     }, ))  dataset = dataset.cache() dataset = dataset.shuffle(BUFFER_SIZE) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)  # 최종 버전 class PositionalEncoding(tf.keras.layers.Layer):     def __init__(self, position, d_model):         super(PositionalEncoding, self).__init__()         self.pos_encoding = self.positional_encoding(position, d_model)      def get_angles(self, position, i, d_model):         angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))         return position * angles      def positional_encoding(self, position, d_model):         angle_rads = self.get_angles(             position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],             i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],             d_model=d_model)          # 배열의 짝수 인덱스(2i)에는 사인 함수 적용         sines = tf.math.sin(angle_rads[:, 0::2])          # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용         cosines = tf.math.cos(angle_rads[:, 1::2])          angle_rads = np.zeros(angle_rads.shape)         angle_rads[:, 0::2] = sines         angle_rads[:, 1::2] = cosines         pos_encoding = tf.constant(angle_rads)         pos_encoding = pos_encoding[tf.newaxis, ...]          print(pos_encoding.shape)         return tf.cast(pos_encoding, tf.float32)      def call(self, inputs):         return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]      def scaled_dot_product_attention(query, key, value, mask):     # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)     # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)     # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)     # padding_mask : (batch_size, 1, 1, key의 문장 길이)      # Q와 K의 곱. 어텐션 스코어 행렬.     matmul_qk = tf.matmul(query, key, transpose_b=True)      # 스케일링     # dk의 루트값으로 나눠준다.     depth = tf.cast(tf.shape(key)[-1], tf.float32)     logits = matmul_qk / tf.math.sqrt(depth)      # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.     # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.     if mask is not None:         logits += (mask * -1e9)      # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.     # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)     attention_weights = tf.nn.softmax(logits, axis=-1)      # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)     output = tf.matmul(attention_weights, value)      return output, attention_weights  class MultiHeadAttention(tf.keras.layers.Layer):    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):     super(MultiHeadAttention, self).__init__(name=name)     self.num_heads = num_heads     self.d_model = d_model      assert d_model % self.num_heads == 0      # d_model을 num_heads로 나눈 값.     # 논문 기준 : 64     self.depth = d_model // self.num_heads      # WQ, WK, WV에 해당하는 밀집층 정의     self.query_dense = tf.keras.layers.Dense(units=d_model)     self.key_dense = tf.keras.layers.Dense(units=d_model)     self.value_dense = tf.keras.layers.Dense(units=d_model)      # WO에 해당하는 밀집층 정의     self.dense = tf.keras.layers.Dense(units=d_model)    # num_heads 개수만큼 q, k, v를 split하는 함수   def split_heads(self, inputs, batch_size):     inputs = tf.reshape(         inputs, shape=(batch_size, -1, self.num_heads, self.depth))     return tf.transpose(inputs, perm=[0, 2, 1, 3])    def call(self, inputs):     query, key, value, mask = inputs['query'], inputs['key'], inputs[         'value'], inputs['mask']     batch_size = tf.shape(query)[0]      # 1. WQ, WK, WV에 해당하는 밀집층 지나기     # q : (batch_size, query의 문장 길이, d_model)     # k : (batch_size, key의 문장 길이, d_model)     # v : (batch_size, value의 문장 길이, d_model)     # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.     query = self.query_dense(query)     key = self.key_dense(key)     value = self.value_dense(value)      # 2. 헤드 나누기     # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)     # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)     # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)     query = self.split_heads(query, batch_size)     key = self.split_heads(key, batch_size)     value = self.split_heads(value, batch_size)      # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.     # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)     scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)     # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)     scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])      # 4. 헤드 연결(concatenate)하기     # (batch_size, query의 문장 길이, d_model)     concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))      # 5. WO에 해당하는 밀집층 지나기     # (batch_size, query의 문장 길이, d_model)     outputs = self.dense(concat_attention)      return outputs  def create_padding_mask(x):   mask = tf.cast(tf.math.equal(x, 0), tf.float32)   # (batch_size, 1, 1, key의 문장 길이)   return mask[:, tf.newaxis, tf.newaxis, :]        def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):   inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")    # 인코더는 패딩 마스크 사용   padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")    # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)   attention = MultiHeadAttention(       d_model, num_heads, name=\"attention\")({           'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V           'mask': padding_mask # 패딩 마스크 사용       })    # 드롭아웃 + 잔차 연결과 층 정규화   attention = tf.keras.layers.Dropout(rate=dropout)(attention)   attention = tf.keras.layers.LayerNormalization(       epsilon=1e-6)(inputs + attention)    # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)   outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)   outputs = tf.keras.layers.Dense(units=d_model)(outputs)    # 드롭아웃 + 잔차 연결과 층 정규화   outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)   outputs = tf.keras.layers.LayerNormalization(       epsilon=1e-6)(attention + outputs)    return tf.keras.Model(       inputs=[inputs, padding_mask], outputs=outputs, name=name)        def encoder(vocab_size, num_layers, dff,             d_model, num_heads, dropout,             name=\"encoder\"):   inputs = tf.keras.Input(shape=(None,), name=\"inputs\")    # 인코더는 패딩 마스크 사용   padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")    # 포지셔널 인코딩 + 드롭아웃   embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)   embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))   embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)   outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)    # 인코더를 num_layers개 쌓기   for i in range(num_layers):     outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,         dropout=dropout, name=\"encoder_layer_{}\".format(i),     )([outputs, padding_mask])    return tf.keras.Model(       inputs=[inputs, padding_mask], outputs=outputs, name=name)        # 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수 def create_look_ahead_mask(x):   seq_len = tf.shape(x)[1]   look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)   padding_mask = create_padding_mask(x) # 패딩 마스크도 포함   return tf.maximum(look_ahead_mask, padding_mask)       def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):   inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")   enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")    # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.   look_ahead_mask = tf.keras.Input(       shape=(1, None, None), name=\"look_ahead_mask\")   padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')    # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)   attention1 = MultiHeadAttention(       d_model, num_heads, name=\"attention_1\")(inputs={           'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V           'mask': look_ahead_mask # 룩어헤드 마스크       })    # 잔차 연결과 층 정규화   attention1 = tf.keras.layers.LayerNormalization(       epsilon=1e-6)(attention1 + inputs)    # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)   attention2 = MultiHeadAttention(       d_model, num_heads, name=\"attention_2\")(inputs={           'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V           'mask': padding_mask # 패딩 마스크       })    # 드롭아웃 + 잔차 연결과 층 정규화   attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)   attention2 = tf.keras.layers.LayerNormalization(       epsilon=1e-6)(attention2 + attention1)    # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)   outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)   outputs = tf.keras.layers.Dense(units=d_model)(outputs)    # 드롭아웃 + 잔차 연결과 층 정규화   outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)   outputs = tf.keras.layers.LayerNormalization(       epsilon=1e-6)(outputs + attention2)    return tf.keras.Model(       inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],       outputs=outputs,       name=name)        def decoder(vocab_size, num_layers, dff,             d_model, num_heads, dropout,             name='decoder'):   inputs = tf.keras.Input(shape=(None,), name='inputs')   enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')    # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.   look_ahead_mask = tf.keras.Input(       shape=(1, None, None), name='look_ahead_mask')   padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')    # 포지셔널 인코딩 + 드롭아웃   embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)   embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))   embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)   outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)    # 디코더를 num_layers개 쌓기   for i in range(num_layers):     outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,         dropout=dropout, name='decoder_layer_{}'.format(i),     )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])    return tf.keras.Model(       inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],       outputs=outputs,       name=name)       def transformer(vocab_size, num_layers, dff,                 d_model, num_heads, dropout,                 name=\"transformer\"):    # 인코더의 입력   inputs = tf.keras.Input(shape=(None,), name=\"inputs\")    # 디코더의 입력   dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")    # 인코더의 패딩 마스크   enc_padding_mask = tf.keras.layers.Lambda(       create_padding_mask, output_shape=(1, 1, None),       name='enc_padding_mask')(inputs)    # 디코더의 룩어헤드 마스크(첫번째 서브층)   look_ahead_mask = tf.keras.layers.Lambda(       create_look_ahead_mask, output_shape=(1, None, None),       name='look_ahead_mask')(dec_inputs)    # 디코더의 패딩 마스크(두번째 서브층)   dec_padding_mask = tf.keras.layers.Lambda(       create_padding_mask, output_shape=(1, 1, None),       name='dec_padding_mask')(inputs)    # 인코더의 출력은 enc_outputs. 디코더로 전달된다.   enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,       d_model=d_model, num_heads=num_heads, dropout=dropout,   )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크    # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.   dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,       d_model=d_model, num_heads=num_heads, dropout=dropout,   )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])    # 다음 단어 예측을 위한 출력층   outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)   small_transformer = transformer(     vocab_size = 9000,     num_layers = 4,     dff = 512,     d_model = 128,     num_heads = 4,     dropout = 0.3,     name=\"small_transformer\")  tf.keras.utils.plot_model(     small_transformer, to_file='small_transformer.png', show_shapes=True)  def loss_function(y_true, y_pred):   y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))    loss = tf.keras.losses.SparseCategoricalCrossentropy(       from_logits=True, reduction='none')(y_true, y_pred)    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)   loss = tf.multiply(loss, mask)    return tf.reduce_mean(loss)   class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):   def __init__(self, d_model, warmup_steps=4000):     super(CustomSchedule, self).__init__()     self.d_model = d_model     self.d_model = tf.cast(self.d_model, tf.float32)     self.warmup_steps = warmup_steps    def __call__(self, step):     arg1 = tf.math.rsqrt(step)     arg2 = step * (self.warmup_steps**-1.5)      return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)  tf.keras.backend.clear_session()  # Hyper-parameters NUM_LAYERS = 2 D_MODEL = 256 NUM_HEADS = 8 DFF = 512 DROPOUT = 0.1  model = transformer(     vocab_size=VOCAB_SIZE,     num_layers=NUM_LAYERS,     dff=DFF,     d_model=D_MODEL,     num_heads=NUM_HEADS,     dropout=DROPOUT)  MAX_LENGTH = 40  learning_rate = CustomSchedule(D_MODEL)  optimizer = tf.keras.optimizers.Adam(     learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)  def accuracy(y_true, y_pred):   # ensure labels have shape (batch_size, MAX_LENGTH - 1)   y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))   return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)  model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])   EPOCHS = 5  model.fit(dataset, epochs=EPOCHS)  def evaluate(sentence):   sentence = preprocess_sentence(sentence)    sentence = tf.expand_dims(       START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)    output = tf.expand_dims(START_TOKEN, 0)    # 디코더의 예측 시작   for i in range(MAX_LENGTH):     predictions = model(inputs=[sentence, output], training=False)      # 현재(마지막) 시점의 예측 단어를 받아온다.     predictions = predictions[:, -1:, :]     predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)      # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단     if tf.equal(predicted_id, END_TOKEN[0]):       break      # 마지막 시점의 예측 단어를 출력에 연결한다.     # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.     output = tf.concat([output, predicted_id], axis=-1)    return tf.squeeze(output, axis=0)  def preprocess_sentence(sentence):   sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)   sentence = sentence.strip()   return sentence  def predict(sentence):   prediction = evaluate(sentence)    predicted_sentence = tokenizer.decode(       [i for i in prediction if i &lt; tokenizer.vocab_size])    print('Input: {}'.format(sentence))   print('Output: {}'.format(predicted_sentence))    return predicted_sentence  output = predict('영화 볼래?')   Reference:      https://jalammar.github.io/illustrated-transformer/   https://arxiv.org/abs/1706.03762  ","categories": ["Deep Learning"],
        "tags": ["Natural Language Processing"],
        "url": "/deeplearning/transformer",
        "teaser": null
      },{
        "title": "[ViT] Transformers For Image Recognition at Scale",
        "excerpt":"Introduction   Attention 계열 구조는 자연어 처리분야에 많이 사용되어 왔다. 하지만 비전 분야에서는 CNN 계열 모델이 우세하게 사용되고 있다. 본 논문에서는 Transformer를 이미지 분류 문제에 적용한 연구 실험을 기술했다.   Model Architecture         전체적인 ViT 모델의 구조는 ‘All You Need Is Attention’ 논문에서 나오는 Transformer Encoder 구조와 비슷하다. 다만 텍스트 형식의 데이터를 사용하지 않고 여러 이미지 패치를 사용한다. 본 논문에서는 $(H, W, C)$ 크기의 이미지를 $N$ 개의$(P, P)$ 패치로 자른 후, 각 패치를 $1D$ sequence 형태인 $P^2 \\cdot C$ 차원의 vector로 만든다. 그리고 BERT의 $[class]$ 토큰과 비슷하게, classification token을 Position Embedding[0]에 더해준 것을 Position Embedding[1:] 과 Patch Embedding을 더해준 것에 concatenate 해준다. 저자는 2D-aware position embeddings도 사용해봤는데 성능 향상에 도움되지 않았다고 한다.   그리고 Transformer Encoder에 Patch + Position Embedding (+ [class] embedding) 값을 입력데이터로 넣어준다. 최종적으로 Linear연산을 통해 classification을 하게 된다. 여기서  Transformer Encoder는 Normalization이 맨 앞으로 온 것을 빼면 똑같은 구조를 사용한다.     Result     ViT Tensorflow 실습 코드   참고: https://dzlab.github.io/notebooks/tensorflow/vision/classification/2021/10/01/vision_transformer.html   import tensorflow as tf from tensorflow.keras.layers import Layer, Dense, Dropout, LayerNormalization, MultiHeadAttention, Add from tensorflow.keras.layers import Input, Embedding, Concatenate from tensorflow.keras.models import Model, Sequential  class Patches(Layer):      # From Keras Examples     def __init__(self, patch_size):         super(Patches, self).__init__()         self.patch_size = patch_size      def call(self, images):         batch_size = tf.shape(images)[0]         patches = tf.image.extract_patches(             images=images,             sizes=[1, self.patch_size, self.patch_size, 1],             strides=[1, self.patch_size, self.patch_size, 1],             rates=[1, 1, 1, 1],             padding=\"VALID\",         )         patch_dims = patches.shape[-1]         patches = tf.reshape(patches, [batch_size, -1, patch_dims])         # print(patches.shape)         return patches  def mlp_head(x, config):     x = Dense(config[\"mlp_head_dim\"] * 2, activation=\"gelu\")(x)     x = Dropout(config[\"dropout_rate\"])(x)     x = Dense(config[\"mlp_head_dim\"])(x)     x = Dropout(config[\"dropout_rate\"])(x)     return x  def transformer_encoder(x, config):     skip_connection_1 = x     x = LayerNormalization()(x)     x = MultiHeadAttention(num_heads=config[\"num_heads\"], key_dim=config[\"embedding_dim\"])(x, x)     x = Add()([x, skip_connection_1])      skip_connection_2 = x     x = LayerNormalization()(x)     x = mlp_head(x, config)     x = Add()([x, skip_connection_2])      return x  class PatchEncoder(Layer):     def __init__(self):         super(PatchEncoder, self).__init__()         self.num_patches = config[\"num_patches\"]         self.projection_dim = config[\"embedding_dim\"]         w_init = tf.random_normal_initializer()         class_token = w_init(shape=(1, self.projection_dim), dtype=\"float32\")         self.class_token = tf.Variable(initial_value=class_token, trainable=True)         self.projection = Dense(units=self.projection_dim)         self.position_embedding = Embedding(input_dim=self.num_patches+1, output_dim=self.projection_dim)      def call(self, patch):         batch = tf.shape(patch)[0]         # reshape the class token embedins         class_token = tf.tile(self.class_token, multiples = (batch, 1))         class_token = tf.reshape(class_token, (batch, 1, self.projection_dim))         # calculate patches embeddings         patches_embedding = self.projection(patch)         patches_embedding = tf.concat([class_token, patches_embedding], 1)         # calcualte positional embeddings         positions = tf.range(start=0, limit=self.num_patches+1, delta=1)         positions_embed = self.position_embedding(positions)         # add both embeddings         encoded = patches_embedding + positions_embed         return encoded  def ViT(config):     # Inputs and Embedding     input_shape = (config[\"img_size\"], config[\"img_size\"], config[\"num_channels\"])     inputs = Input(input_shape)     p = Patches(config['patch_size'])(inputs)     x = PatchEncoder()(p)       # Encoder     for _ in range(config[\"num_layers\"]):         x = transformer_encoder(x, config)      # Classification head     x = LayerNormalization()(x)     x = x[:, 0, :]     x = Dense(config[\"num_classes\"], activation = \"softmax\")(x)      model = Model(inputs, x)     return model   if __name__ == \"__main__\":     config = {}     config[\"embedding_dim\"] = 64     config[\"mlp_head_dim\"] = 64     config[\"dropout_rate\"] = 0.1     config[\"num_heads\"] = 4     config[\"num_classes\"] = 10     config[\"num_patches\"] = 256         config[\"num_layers\"] = 8         config[\"img_size\"] = 256     config[\"patch_size\"] = 16     config[\"num_channels\"] = 3     config[\"num_classes\"] = 10      model = ViT(config)     img = tf.random.normal(shape=[1, config[\"img_size\"], config[\"img_size\"] , 3])     preds = model(img)     print(preds)  ","categories": ["Deep Learning"],
        "tags": ["Computer Vision"],
        "url": "/deeplearning/vit",
        "teaser": null
      },{
        "title": "[AutoRec] Autoencoders Meet Collaborative Filtering",
        "excerpt":"AutoRec 모델         AutoRec 모델은 Auto-Encoder 구조를 사용하고, 사용자 또는 아이템중 하나로Latent Feature를 만들어 Rating Matrix Completion을 수행한다. 본 논문에서는 아이템을 임베딩하는 모델을 I-AutoRec, 사용자를 임베딩하는 모델을 U-AutoRec 라고 부른다.   사용자(m 명)-아이템 (n 개) 평점 행렬 $R \\in \\mathbb{R}^{m \\times n}$ 이 있다고 가정한다. AutoRec 은 입력값 $\\mathbf{r^{u}} \\text{ or } \\mathbf{r^{i}}\\in \\mathbb{R}^{d}$ 를 받아, 이를 복원하는 $h(\\mathbf{r^{z}};\\theta)$ 를 다음과 같이 정의한다.   \\[h(\\mathbf{r^{z}}; \\theta) = f(\\mathbf{W} \\cdot g(\\mathbf{Vr^{z}} + \\boldsymbol{\\mu}) + \\mathbf{b}) \\\\ \\text{where z could be either be } \\mathbf{u} \\text{ or } \\mathbf{i}\\]  위 식(1)에서 $f(\\cdot)$ 과 $g(\\cdot)$ 는 각각 decoder와 encoder의 활성화 함수이다. 본 논문에서는 I-AutoRec를 사용했을 때, identify function을 encoder의 활성화 함수로, sigmoid function을 decoder의 활성화 함수로 사용했을 때 성능 (RMSE)이 가장 좋았다고 한다. 그리고 마지막으로 AutoRec 에서는 다음과 같은 목적함수를 사용한다. 여기서 목적함수를 계산할 때 observed ratings $\\mathcal{{O}}$만 고려한다는 것이다.   \\[\\min_\\theta \\sum^n_{z=1} \\| \\mathbf{r}^{(z)} - h(\\mathbf{r}^{(z)}; \\theta) \\|^2_\\mathcal{O} + \\frac{\\lambda}{2} \\left( \\| \\mathbf{W}_F^2 \\| + \\| \\mathbf{V} \\|^2_F \\right)\\]  Result     AutoRec Tensorflow Implementation   import tensorflow as tf from tensorflow.keras.layers import Dense, Input, Layer from tensorflow.keras import Model, Sequential, regularizers, optimizers, metrics  from zipfile import ZipFile from pathlib import Path import pandas as pd import numpy as np from sklearn.model_selection import train_test_split  class Encoder(Layer):     def __init__(self, num_hidden):         super(Encoder, self).__init__()         self.n_dims = num_hidden         self.encoder_layer = Dense(self.n_dims, activation = None, kernel_regularizer=regularizers.l2(0.01))          @tf.function     def call(self, inputs):         return self.encoder_layer(inputs)      class Decoder(Layer):     def __init__(self, num_reconstruction):         super(Decoder, self).__init__()         self.n_dims = num_reconstruction         self.decoder_layer = Dense(self.n_dims, activation = 'sigmoid')         # self.decoder_layer = Dense(self.n_dims)              @tf.function     def call(self, inputs):         x = self.decoder_layer(inputs)         return x  class AutoRec(Model):     def __init__(self, num_hidden, num_reconstruction):         super(AutoRec, self).__init__()         self.encoder = Encoder(num_hidden)         self.decoder = Decoder(num_reconstruction)      @tf.function            def call(self, inputs):         x = self.encoder(inputs)         x = self.decoder(x)         return x      def ObservedOnlyMSELoss(y_true, y_pred):     # 참고: https://supkoon.tistory.com/36     mask = y_true != 0     mask_float = tf.cast(mask, tf.float32)     masked_error = tf.reduce_mean(tf.pow(tf.subtract(mask_float * y_pred,y_true),2))     return masked_error   movielens_data_file_url = (     \"http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\" ) movielens_zipped_file = tf.keras.utils.get_file(     \"ml-latest-small.zip\", movielens_data_file_url, extract=False ) keras_datasets_path = Path(movielens_zipped_file).parents[0] movielens_dir = keras_datasets_path / \"ml-latest-small\"  # Only extract the data the first time the script is run. if not movielens_dir.exists():     with ZipFile(movielens_zipped_file, \"r\")as zip:         # Extract files         print(\"Extracting all the files now...\")         zip.extractall(path=keras_datasets_path)         print(\"Done!\")  class dataloader():     # 참고: https://github.com/supkoon/AutoRec-tf/blob/master/AutoRec.py     def __init__(self,test_size, path = movielens_dir / \"ratings.csv\"):         self.test_size = test_size         self.ratings_df = pd.read_csv(path)         self.ratings_df.columns = [\"userId\",\"movieId\",\"rating\",\"timestamp\"]         self.num_user = len(self.ratings_df.userId.unique())         self.num_item = len(self.ratings_df.movieId.unique())              def make_user_autorec_input(self):         user_item_df = self.ratings_df.pivot_table(values=\"rating\", index=\"userId\", columns=\"movieId\")         user_item_df.fillna(0,inplace=True)         self.user_item_df = np.array(user_item_df)         train_df,test_df = train_test_split(self.user_item_df, test_size =self.test_size)         return train_df,test_df      def make_item_autorec_input(self):         item_user_df = self.ratings_df.pivot_table(values=\"rating\", index=\"movieId\", columns=\"userId\")         item_user_df.fillna(0,inplace=True)         self.item_user_df = np.array(item_user_df)         train_df,test_df = train_test_split(self.item_user_df, test_size =self.test_size)         return train_df,test_df      dataloader = dataloader(0.1)    train_data, test_data = dataloader.make_item_autorec_input() num_features = dataloader.num_user  model = AutoRec(num_features // 2, num_features) model.compile(optimizer=optimizers.SGD(learning_rate=0.001), loss= ObservedOnlyMSELoss, metrics = [metrics.RootMeanSquaredError()]) model.fit(train_data, train_data, batch_size=16, epochs=10, validation_data=(test_data, test_data))   Reference:      http://users.cecs.anu.edu.au/~u5098633/papers/www15.pdf   https://keras.io/examples/structured_data/collaborative_filtering_movielens/  ","categories": ["Deep Learning"],
        "tags": ["Recommender System"],
        "url": "/deeplearning/autorec",
        "teaser": null
      },{
        "title": "[BERT] Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "excerpt":"Introduction   BERT(Bidirectional Encoder Representations from Transformers)는 2018년에 구글 리서치 팀에 의해 공개된 Language Representation 모델이다. 이름에서 알 수 있듯이 BERT 모델은 양방향성 (Bidirectional) 특성을 갖고 있다. 그렇다면 양방형 문맥을 고려한다는 것에 무슨 의미이고, 어떤 장점이 있을까? 양방향으로 학습한다는 것은 전체적인 문맥을 파악하기 위함이다. 직관적으로 생각해보면 단방향에서 오는 정보보다 양방향에서 오는 정보가 많기 때문에 정보의 질에 차이가 날 수 밖에 없다. 예를 들어, “She is eating a bowl of salad”라는 문장이 있을 때, “eat”라는 동사를 정해놓고 “salad”를 사용하지 않는다. “salad”를 놓고 어떤 액션을 하고 있다는 말을 하고 싶었을 수도 있다. 이렇듯 양방향으로 학습하면 전체적인 문맥을 이해할 수 있다.   BERT Architecture       BERT는 Transformer의 Encoder 부분만 사용한다. BERT는 구조의 크기에 따라 Base와 Large 2가지 유형의 모델로 나눠진다. BERT-Base 모델의 Hyperparameter는 $L = 12$, $H = 768$, $A = 12$ 이고 BERT-Large 모델의 Hyperparameter는 $L = 24$, $H = 1024$, $A = 16$ 이다.      L = # Transformer Block   H = # Hidden Layer   A = # Self Attention Head   더하여 BERT는 기존의 자연어처리 사전학습 모델의 문제점을 보완하기 위해 두 가지 unsupervised tasks: (1) Masked language model, (2) next sentence prediction (NSP) 방법을 사용해 학습한다.       Masked Language Model (MLM)   MLM 는 [Mask]된 단어를 예측하면서 전체적인 문맥을 파악하는 능력을 학습한다. MLM 수행 과정은 다음과 같다. 우선 입력 데이터의 토큰 중 15%는 무작위로 선택한다. 선택된 토큰중 80% [Mask] 토큰으로, 10%는 랜덤한 단어로 바뀐다. 그리고 나머지 10%는 오리지널한 단어 그 상태 그대로 유지된다.   Next Sentence Prediction (NSP)   NSP는 두 번째 문장이 첫 번째 문장 다음으로 오는 문장인지 맞추는 문제를 푼다. 첫 번째 문장과 두 번째 문장은 [SEP]로 구분한다. 두 번째 문장이 첫 번째 문장을 연속하는지는 50% 비율로 참인 문장과, 50%의 랜덤하게 추출된 문장으로 구성해 학습한다. 이 과정을 통해 문맥과 문장의 관계를 학습할 수 있다.   BERT Input           위 그림처럼 세 가지 임베딩(Token, Segment, Position)을 사용해서 문장을 표현한다.      Token Embedding: 모든 문장의 시작을 표현하는 특수 토큰 [CLS], 문장을 구분하기 위한 특수 토큰 [SEP], 그리고 단어별 임베딩으로 구성   Segment Embedding: 문장을 구분하기 위한 임베딩   Position Embedding: Transformer 구조에서 사용된 토큰의 위치를  알려주는 임베딩   이 세 가지 임베딩을 더한 임베딩을 입력 데이터로 사용하게 된다.   BERT, GPT, ELMo Comparison       Results   GLUE 데이터셋에 대한 BERT 실험 결과       BERT Tensorflow Code Example   https://www.tensorflow.org/text/tutorials/classify_text_with_bert   # !pip install -q -U \"tensorflow-text==2.8.*\" # !pip install -q tf-models-official==2.7.0 import os import shutil  import tensorflow as tf import tensorflow_hub as hub import tensorflow_text as text from official.nlp import optimization  # to create AdamW optimizer  import matplotlib.pyplot as plt  tf.get_logger().setLevel('ERROR')  url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'  dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,                                   untar=True, cache_dir='.',                                   cache_subdir='')  dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')  train_dir = os.path.join(dataset_dir, 'train')  # remove unused folders to make it easier to load the data remove_dir = os.path.join(train_dir, 'unsup') shutil.rmtree(remove_dir)  AUTOTUNE = tf.data.AUTOTUNE batch_size = 32 seed = 42  raw_train_ds = tf.keras.utils.text_dataset_from_directory(     'aclImdb/train',     batch_size=batch_size,     validation_split=0.2,     subset='training',     seed=seed)  class_names = raw_train_ds.class_names train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)  val_ds = tf.keras.utils.text_dataset_from_directory(     'aclImdb/train',     batch_size=batch_size,     validation_split=0.2,     subset='validation',     seed=seed)  val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)  test_ds = tf.keras.utils.text_dataset_from_directory(     'aclImdb/test',     batch_size=batch_size)  test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)  tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1' tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3' bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) bert_model = hub.KerasLayer(tfhub_handle_encoder)  def build_classifier_model():   text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')   preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')   encoder_inputs = preprocessing_layer(text_input)   encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')   outputs = encoder(encoder_inputs)   net = outputs['pooled_output'] # CLS   net = tf.keras.layers.Dropout(0.1)(net)   net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)   return tf.keras.Model(text_input, net)  classifier_model = build_classifier_model() loss = tf.keras.losses.BinaryCrossentropy(from_logits=True) metrics = tf.metrics.BinaryAccuracy()  epochs = 5 steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy() num_train_steps = steps_per_epoch * epochs num_warmup_steps = int(0.1*num_train_steps)  init_lr = 3e-5 optimizer = optimization.create_optimizer(init_lr=init_lr,                                           num_train_steps=num_train_steps,                                           num_warmup_steps=num_warmup_steps,                                           optimizer_type='adamw')  classifier_model.compile(optimizer=optimizer,                          loss=loss,                          metrics=metrics)  history = classifier_model.fit(x=train_ds,                                validation_data=val_ds,                                epochs=epochs)  examples = [     'this is such an amazing movie!',  # this is the same sentence tried earlier     'The movie was great!',     'The movie was meh.',     'The movie was okish.',     'The movie was terrible...' ]  original_results = tf.sigmoid(classifier_model(tf.constant(examples)))  print('Results from the model in memory:') print_my_examples(examples, original_results)   Reference:      https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270   https://hwiyong.tistory.com/392   https://keep-steady.tistory.com/19  ","categories": ["Deep Learning"],
        "tags": ["Natural Language Processing"],
        "url": "/deeplearning/bert",
        "teaser": null
      },{
        "title": "[NeuMF] Neural Collaborative Filtering",
        "excerpt":"Introduction   넷플릭스 경진대회의해 알려진 Matrix Factorization은 추천시스템 분야에서 널리 사용되는 방법이다. 이는 사용자와 아이템이 상호작용하는 Latent Matrix을 Inner Product를 통해 사용자의 Latent Matrix과 아이템의 Latent Matrix로 분해한다. 본 논문에서는 Inner Product 기반의 Matrix Factorization은 선형적인 관계만 모델링한다는 한계에 대해 지적하고, 사용자와 아이템간의 관계를 더 잘 표현할 수 있는 신경망 기반의 NeuMF 모델을 제안했다.   Matrix Factorization 문제점     위 그림(a)은 사용자(row)-아이템(column) 관계를 행렬로 표현하고 있다. 여기서 $y_{u,i}=1$은 user $u$ 와 item $i$ 간의 상호작용이 있었음을 나타낸다. 상호작용이란 사용자가 아이템을 확인했거나, 구매했다는 등의 implicit 한 정보를 의마한다. 따라서 $y_{u,i}=0$ 은 user $u$ 와 item $i$ 간의 상호작용이 없었다는 뜻이지, user $u$ 가 item $i$ 를 선호하지 않는다는 뜻은 아니다.   Inner Product 기반의 Matrix Factorization에 어떤 문제가 있는지 자카드 유사도(Jaccard Similarity) 를 고려하는 경우를 가정한다.그러면 위 그림(a)와 같은 행렬로부터 다음과 같은 관계가 성립한다고 볼 수 있다.   \\[s_{23}(0.66) &gt; s_{12}(0.5) &gt; s_{13}(0.4)\\]  즉, 사용자 2와 사용자 3이 사용자 1과 사용자 2 보다 비슷하고, 사용자 1과 사용자 2이 사용자 1과 사용자 3 보다 비슷하다는 뜻이다. 위 그림 (b) 는 이런 관계를 기하학적으로 보여주고 있다. Matrix Factorization 의 한계는 사용자 4가 등장했을 때 발생한다. 사용자 4와 나머지 사용자의 자카드 유사도 관계는 다음과 같다.   \\[s_{41}(0.6) &gt; s_{43}(0.4) &gt; s_{42}(0.2)\\]  하지만 그림 (b)에  $p_4$를 어디에 놔도 $p_3$보다 $p_2$가 더 가깝기 때문에 ranking loss가 커질 수 밖에 없다. 이런 한계는 사용자와 아이템의 관계를 저차원의 공간에 표현 하는 데에서 기인한다. 따라서 본 논문에서는 사용자와 아이템의 상호작용을 더 복잡한 차원에서 표현할 수 있도록 신표현할 수 있도록 신경망을 활용해 해결하고자 했다.   Neural Collaborative Filtering Framework     본 논문에서 제안한 Neural Collaboraitive Filtering의 General Framework 는 총 4개의 레이어로 구성되었다: (1) Input Layer, (2) Embedding Layers, (3) Neural CF Layers, 그리고 (4) Output Layers.   Input Layer는 각각 사용자($v_u^U$)와 아이템($v_i^I$)을 나타내는 원핫인코디드된 Feature vector로 구성되어 있다. Embedding Layer 에서 Sparse한 이 Feature vector를 Dense한 Latent vector로 바꿔주는 역할을 한다. 임베딩이 된 사용자와 아이템 Latent vector를 concatenation한 vector를 Neural CF Layers에 들어가게 되고 복잡하고 비선형적인 데이터 관계를 학습하게 된다. 마지막으로 Output Layers에서 사용자 $u$와 아이템 $i$가 얼마나 관련 있는지를 나타내는 $\\hat{y_{u,i}}$ 값을 계산한다.   Generalized Matrix Factorization (GMF)   저자는 Matrix Factorization 역시 NCF framework의 특수한 케이스가 됨을 보여주고 이를 GMF라고 한다. Latent Vector $p_u$ ($P^Tv^U_u$), $q_i$ ($Q^Tv^I_i$) 라고 정의했을 때, 첫번째 NCF layer의 mapping function을 다음과 같다.   \\[\\phi_1(p_u,q_i) = p_u\\odot q_i\\]  이 결과를 output layer에 project한다면 아래와 같이 표현할 수 있다. 여기서 $a_{out}$ 를 identical function으로 가정하고, $h$를 uniform vector 1로 정의한다면, 기존 Matrix Factorization과 동일해집니다.   \\[\\hat{y}_{ui} = a_{out}(h^T(p_u \\odot q_i))\\]  GMF란 $a_{out}$ 와 $h$를 아래와 같이 두어 Matrix Factorization를 일반화한 모델이다.   \\[a_{out} = \\frac{1}{1 + e^{−x}},\\ h^T = [h_1 , ... , h_k],\\]  Multi-Layer Perceptron (MLP)   GMF의 fixed/linear (element-wise product)한 특징으로 인해 사용자와 아이템간의 복잡한 관계를 표현하지 못하고, MLP는 flexible/non-linear하기 때문에 복잡한 관계를 표현할 수 있다.   \\[z_1 = \\phi_1(p_u,q_i) = \\begin{bmatrix}p_u\\\\q_i\\end{bmatrix},\\\\ \\phi_2(z_1) = a_2(W_2^Tz_1+b_2), \\\\ ... \\\\ \\phi_L(z_{L-1}) = a_L(W_L^Tz_{L-1}+b_L), \\\\ \\hat{y}_{ui} = \\sigma(h^T\\phi_L(Z_{L-1}))\\]  Fusion of GMF and MLP   본 논문에서는 GMF와 MLP를 통합한 모델은 제안한다.   \\[\\phi^{GMF} = p_{u}^{G} \\odot q_{i}^{G}, \\\\ \\phi^{MLP} = a_{L}(W_{L}^{T}(a_{L-1}(...a_{2}(W_{2}^{T} \\begin{bmatrix} p_{u}^{M} \\\\ q_{i}^{M} \\end{bmatrix}+b_{2})...))+b_{L}), \\\\ \\hat{y}_{u,i} = \\sigma(h^{T} \\begin{bmatrix}\\phi^{GMF} \\\\ \\phi^{MLP} \\end{bmatrix})\\]  $p^G_u$와 $q^G_i$는 GMF를 위한 embedding이고 $p^M_u$와 $q^M_i$는 MLP를 위한 embedding이다. 그리고 $a_L$ 활성화 함수로 ReLU를 사용했다고 한다.   Result     NMF 모델 Tensorflow 실습   class NeuMF(Model):     def __init__(self, user_num, item_num, latent_features = 8, alpha = 0.5):         super(NeuMF, self).__init__()         self.latent_features = latent_features         self.user_num = user_num         self.item_num = item_num         self.alpha = alpha                  self.gmf_embedding_user = Embedding(input_dim = self.user_num, output_dim = self.latent_features)         self.gmf_embedding_item = Embedding(input_dim = self.item_num, output_dim = self.latent_features)         self.mlp_embedding_user = Embedding(input_dim = self.user_num, output_dim = 32)         self.mlp_embedding_item = Embedding(input_dim = self.item_num, output_dim = 32)                  self.mlp_vector1 = Dense(units=16, activation='relu')         self.mlp_vector2 = Dense(units=8, activation='relu')                  self.prediction = Dense(1, activation='sigmoid')              def call(self, inputs):         user_input, item_input = inputs          # Embedding layer         gmf_embedding_user = self.gmf_embedding_user(user_input)         gmf_embedding_item = self.gmf_embedding_user(item_input)         mlp_embedding_user = self.gmf_embedding_user(user_input)         mlp_embedding_item = self.gmf_embedding_user(item_input)          # GMF part         gmf_user_latent = Flatten()(gmf_embedding_user)         gmf_item_latent = Flatten()(gmf_embedding_item)         gmf_vector = Multiply()([gmf_user_latent, gmf_item_latent])                   # MLP part          mlp_user_latent = Flatten()(mlp_embedding_user)         mlp_item_latent = Flatten()(mlp_embedding_item)         mlp_vector = Concatenate()([mlp_user_latent, mlp_item_latent])                  mlp_vector1 = self.mlp_vector1(mlp_vector)         mlp_vector2 = self.mlp_vector2(mlp_vector1)                  # Concatenate GMF and MLP parts         gmf_vector = Lambda(lambda x: x * self.alpha)(gmf_vector)         mlp_vector2 = Lambda(lambda x : x * (1-self.alpha))(mlp_vector2)         prediction_vector = Concatenate()([gmf_vector, mlp_vector2])                  # Prediction Layer         return self.prediction(prediction_vector)  def train_instances(uids, iids, num_neg, num_items):     user_input, item_input, labels = [],[],[]     zipped = set(zip(uids, iids)) # train (user, item) 세트      for (u, i) in zip(uids, iids):          # pos item         user_input.append(u)          item_input.append(i)           labels.append(1)             # neg item         for t in range(num_neg):              j = np.random.randint(num_items)              while (u, j) in zipped:                  j = np.random.randint(num_items)               user_input.append(u)  # [u1, u1,  u1,  ...]             item_input.append(j)  # [pos_i, neg_j1, neg_j2, ...]             labels.append(0)      # [1, 0,  0,  ...]      user_input = np.array(user_input).reshape(-1, 1)     item_input = np.array(item_input).reshape(-1, 1)     labels = np.array(labels).reshape(-1, 1)     return user_input, item_input, labels  num_neg = 4 # train_user_ids: 학습 데이터의 유저 아이디 (unique) # train_item_ids: 학습 데이터의 아이템 아이디 (unique) # items: 학습 + 테스트의 아이템 아이디 train_user_ids, train_item_ids, items = load_dataset() # 로드데이터 각자 구현 필요  user_input, item_input, labels = train_instances(train_user_ids, train_item_ids, num_neg, len(items))  model = NeuMF(len(users), len(items))  model.compile(optimizer= 'adam', loss='binary_crossentropy') model.fit([user_input, item_input],labels, batch_size=128, epochs=10, shuffle=True)   Reference:      https://github.com/ngduyanhece/neuMF/blob/master/NeuMF.py   https://leehyejin91.github.io/post-ncf/   https://supkoon.tistory.com/28  ","categories": ["Deep Learning"],
        "tags": ["Recommender System"],
        "url": "/deeplearning/neuralcf",
        "teaser": null
      },{
        "title": "[FM] Factorization Machines",
        "excerpt":"Introduction   본 논문에서는 희소성이 높은 데이터 환경에서도 realiable parameter를 추정할 수 있는 Factorization Machine (FM) 모델을 소개한다. FM 알고리즘은 Support Vector Machine의 장점과 Factorization Model의 장점만을 결합한 모델이다. 따라서 FM 모델의 장점을 다음과 같이 정리할 수 있다.      Highly Sparse data = 희소한 데이터 환경에서 파라미터 추정이 가능하다 (Factorization Model’s 장점).   Linear Complexity = Feature 개수에 따른 선형 복잡도를 갖는다.   General Predictor = 실수나 정수값을 입력데이터로 넣을 수 있다 (SVM’s 장점).   Example: Feature Representation        위 그림과 같은 데이터가 있다고 가정해보자. 그림에서의 파란색 영역은 유저를 의미하는 변수이고, 주황색 영역은 아이템(여기서는 영화 이름)을 나타내는 변수이다. 노란색 영역은 사용자가 다른 영화들에 평점을 준 변수이고, 녹색은 1월 2009년 이후 월 단위 시간을 나타낸다. 마지막으로 붉은색 영역은 해당 영화를 평가하기 전에 평가한 영화를 나태내는 변수이다. 그리고 맨 오른쪽 열은 영화에 대한 평점이다.   예를 들어, 평점을 예측하기 위해 Alice(A)와 StarTrek(ST)사이의 상호작용을 estimate한다고 가정해보자. 인수분해된 상호작용 파라미터인 $&lt;V_{A}, V_{ST}&gt;$를 통해 상호작용을 측정할 수 있다. 우선, Bob(B)과 Charlie(C) 모두 StarWars(SW)에 대한 평점을 각자 4점, 5점 주었기 때문에 유사한 Factor Vector $V_B$, $V_C$를 가진다. 이것은 $&lt;V_{B}, V_{SW}&gt;$와 $&lt;V_{C}, V_{SW}&gt;$가 유사하다는 것을 의미한다. 그리고 Alice는 Titanic(TI)에 5점, Charlie는 1점을 주었기 때문에 Alice와 Charlie는 다른 Factor Vector를 가진다. 그리고 Bob이 StarTrek과 StarWars에 유사한 높은 점수 각각 4점, 5점를 주었기 때문에 두 영화의 Factor Vector는 유사한 상호작용을 가질 것이다. 결론을 말하자면, Alice는 StarTrek에 대해 평점을 낮게 줄 가능성이 있으며, 이를 통해 Alice와 StarTrek에 대한 Factor Vector의 내적이 Alice와 StarWars의 Factor Vector의 내적값과 유사하다는 점을 추측할 수 있다.   Model Equation  degree d=2인 FM 알고리즘 방정식은 다음과 같다.   \\[\\hat{y}(x) := w_{0} + \\sum_{i=1}^{n} w_{i}x_{i} + \\sum_{i=1}^{n}\\sum_{j= i + 1}^{n} &lt;v_i,v_j&gt;x_ix_j \\\\ \\text{where the model parameters that have to be estimated are:} \\\\ w_{0} \\in \\mathbb{R}, \\mathbf{w} \\in \\mathbb{R}^n, \\mathbf{V} \\in \\mathbb{R}^{n \\times k}\\]  위 식의 첫 번째 항 $w_{0}$은 global bias이다. 두번째 항을 보면 $w_{i}$는 i번째 개별 Feature에 대한 가중치이며, $x_{i}$는 하나의 Feature Vector를 의미한다. 모델링을 통해 개별 Feature의 영향력(w)를 estimate하는 것이라고 볼 수 있다. 하지만, Feature들끼리에 대한 상호작용을 고려할 수 없다는 단점이 있어서 세 번째 항인 $&lt;V_i, V_j&gt;$ 이 추가되었으며 이는 i번째와 j번째 Feature간의 상호작용을 의미한다. 중요한 것은 $w_{i,j}$를 사용하는 것이 아니라 이를 k차원으로 인수분해된 두 vector의 내적를 $&lt;V_i, V_j&gt;$로 표현한 것이다. 이것은 변수간 상호작용의 Latent Vector 이다.   \\[&lt;v_i, v_j&gt; := \\sum_{f = 1}^{k}v_{i,f} \\cdot v_{j, f}\\]  $v_i$는 V 내부의 행을 의미하고 k개의 factor를 가진 i번째 변수이며, k는 factorization의 차원이다. 이는 Latent Vector 조합을 모두 고련한다는 것을 의미한다.   Linear Complexity 으로 만드는 식 과정은 다음과 같다.    FM 알고리즘 Tensorflow   # https://github.com/supkoon/factorization_machine_tf class FM(keras.Model):     def __init__(self, n_factor=8, **kwargs):         super().__init__(**kwargs)          self.w_0 = tf.Variable([0.0])         self.w = tf.Variable(tf.zeros(shape=[n]))         self.v = tf.Variable(tf.random.normal(shape=(n, n_factor)))      def call(self,inputs):         degree_1 = tf.math.reduce_sum(tf.multiply(self.w, inputs), axis=1)          degree_2 = 0.5 * tf.math.reduce_sum(             tf.math.pow(tf.linalg.matmul(inputs, self.v), 2)             - tf.linalg.matmul(tf.math.pow(inputs, 2), tf.math.pow(self.v, 2))             , 1             , keepdims=False         )          predict = tf.math.sigmoid(self.w_0 + degree_1 + degree_2)          return predict   Reference:     https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf   https://www.intelligencelabs.tech/c4d971e3-09a5-4e20-9d82-cc623344602d   ","categories": ["Machine Learning"],
        "tags": ["Recommender System"],
        "url": "/machinelearning/fm",
        "teaser": null
      },{
        "title": "[U-Net] Convolutional Networks for Biomedical Image Segmentation",
        "excerpt":"Introduction   이미지를 수집하고 라벨링은 하는 작업은 고된 과정이다. 특히 컴퓨터 비전에서의 분할 문제는 각 픽셀이 어떤 클래스로 분류되는지 알아야 하기 때문에 구하기가 어렵다. 본 논문에서는 데이터 수가 적어도 분할 문제를 풀 수 있는 U자형 구조 U-Net를 소개한다. 이 네트워크 구조는 수축 경로 (contracting path) 그리고 확장 경로 (expanding path)로 크게 나눠진다. U-Net은 contracting path를 통해 context 정보를 구하고, expanding path를 통해 다시 원본 이미지 크기에 segmentation (precise localization) 작업을 수행한다.   Architecture:     Contracting Path  수축 경로는 U-Net 구조에서의 시작 부분(Concave Up, Decreasing)이라고 생각하면 된다. 이미지의 공간해상도를 줄이기 위한 $2 \\times 2$ Max Pooling (Stride = 2)과, ReLU 활성화함수를 포함한 두 번의 $3 \\times 3$ Conv (Stride = 1, No Padding) 연산을 한다. 즉, $3 \\times 3$ Conv (파란색 화살표) $\\rightarrow$ ReLU 활성화 함수 $\\rightarrow$ $2 \\times 2$ Max Pooling (Stride = 2, 빨간색 화살표)를 각 레벨에서 두 번씩 진행하며, 공간해상도는 줄이고 채널의 개수는 2배로 증가시키는 작업을 반복적으로 진행한다.   Expanding Path  확장 경로는 U-Net 구조에서의 뒷부분(Concave Up, Increasing)에 해당된다. 확장 경로에서는 수축 경로에서와는 다르게 이미의 공간해상도를 증가시키기 위한 연산을 수행한다. 수축 경로에서 추출한 특성맵과 concatenation를 진행한 뒤, ReLU 활성화 함수를 포함한 $2 \\times 2$ Up Convolution 적용한다. 즉, Feature Map Concatenation (회색 화살표) $\\rightarrow$ 2번의 $2 \\times 2$ Up Convolution (초록색 화살표) with ReLU를 반복적으로 수행하며 공간해상도는 다시 늘리는 작업을 한다.   마지막 레이어에서는 $1 \\times 1$ Conv filter (청녹색 화살표)를 이용해 클래스의 개수만큼 채널의 개수를 남겨준다.   Data Augmentation  Data Augmentation은 invariance와 robustness 성질을 학습하기 위한 필수적인 요소이다. 본 논문에서는 Random Elastic Deformation을 사용한 것이 분할 네트워크를 학습하는 데에 있어 Key Concept 역할을 했다고 한다. 이름에서 알 수 있듯이 이 증강 기법은 이미지를 유연하게 변형시켜 흔들림이나 왜곡된 이미지를 잘 구분할 수 있게 만든다.   Why U-Net      적은 수의 학습 데이터로도 Biomedical Image Segmentation 문제에서 우수한 성능을 보임.   수축 경로를 거치면서 Context 정보를 구하고, 확장 경로를 통해 정확한 Localization이 가능하도록 설계됨.   파이프라인 네트워크가 없는 End-to-End 구조로 되어 있음.   Results         U-Net Tensorflow Implementation   # padding = 'same' is used for convenience from tensorflow.keras.layers import Conv2D, Activation, Concatenate from tensorflow.keras.layers import MaxPooling2D, Conv2DTranspose, Input from tensorflow.keras import Model   def conv_blocks(inputs, num_filters):     x = Conv2D(num_filters, 3, padding='same')(inputs)     x = Activation(\"relu\")(x)          x = Conv2D(num_filters, 3, padding='same')(x)     x = Activation(\"relu\")(x)     return x  def contracting_block(inputs, num_filters):     x = conv_blocks(inputs, num_filters)     p = MaxPooling2D((2, 2))(x)     return x, p  def expanding_block(inputs, skip_features, num_filters):     x = Conv2DTranspose(num_filters, (2, 2), strides = 2, padding = \"same\")(inputs)     x = Concatenate()([x, skip_features])     x = conv_blocks(x, num_filters)     return x  def build_unet(input_shape):     inputs = Input(input_shape)          s1, p1 = contracting_block(inputs, 64)     s2, p2 = contracting_block(p1, 128)     s3, p3 = contracting_block(p2, 256)     s4, p4 = contracting_block(p3, 512)          b1 = conv_blocks(p4, 1024)          d1 = expanding_block(b1, s4, 512)     d2 = expanding_block(d1, s3, 256)     d3 = expanding_block(d2, s2, 128)     d4 = expanding_block(d3, s1, 64)          outputs = Conv2D(1, (1, 1), padding = \"same\", activation='sigmoid')(d4)     model = Model(inputs, outputs, name = 'unet')     return model  input_shape = (512, 512, 3) model = build_unet(input_shape) model.summary()   Reference:     https://arxiv.org/abs/1505.0459   https://goeden.tistory.com/74  ","categories": ["Deep Learning"],
        "tags": ["Computer Vision"],
        "url": "/deeplearning/unet",
        "teaser": null
      },{
        "title": "[Faster R-CNN] Towards Real-Time Object Detection with Region Proposal Networks",
        "excerpt":" ","categories": ["Deep Learning"],
        "tags": ["Computer Vision"],
        "url": "/deeplearning/fasterrcnn",
        "teaser": null
      },{
        "title": "[USAD] UnSupervised Anomaly Detection on Multivariate Time Series",
        "excerpt":"Introduction   USAD 모델의 구조를 설명하기 앞서 Auto-encoder(AE)와 GAN 에서의 이상탐지의 장단점을 아는 것이 중요하다. 먼저 AE 기반의 이상탐지는 학습 단계와 탐지 단계로 구분할 수 있다. 학습 단계에서는 모델이 정상 데이터를 압축과 복원하는 과정을 거치면서 복원된 데이터와 원본 데이터 간의 차이(reconstruction error)를 최소화하도록 만들어지고, 탐지 단계에서는 비정상과 정상 데이터 모두 넣어 특정 threshold (aka anomaly score)을 넘기면 비정상, 넘기지 못하면 정상으로 이상여부를 판단한다. 하지만 AE는 압축하는 과정에서 불필요한 노이즈를 제거하기 때문에 비정상 데이터가 정상 데이터와 거의 비슷하다면 reconstruction error는 작아질 것이고, 비정상으로 감지되지 못할 것 이다.   GAN 기반의 이상탐지 경우 입력데이터를 압축하고 복원하는 것은 Generator가 담당한다. 이때 Generator의 주된 목표는 Discriminator가 구분하지 못할 정도로 실제데이터와 유사한 데이터를 지속적으로 만드는 것이기 때문에 Generator에서의 Encoder와 Decoder는 비정상과 정상 데이터 정보를 모두 가지고 있다. 그리고 Discriminator는 Generator가 생성데이터가 비정상인지 정상인지 구별하는 역할을 한다. 하지만 GAN 모델도 완벽한 것은 아니다. GAN은 mode-collapse와 non-convergence를 단점으로 가지고 있어 불안정적인 면도 있다.   이 문제를 해결하고 GAN과 AE 장점만으로 만든 이상 탐지 모델이 USAD이다.   Architecture    USAD 는 Phase 1 (AE Training) 과 Phase 2 (Adversarial Training) 단계로 이뤄져있다. 첫 번째 단계에서는 각각의 AE를 원래의 입력으로 잘 복원되도록 학습하는 것이다.실제데이터 W는 인코더 E에 의해 Latent Space Z로 압축되고 각각의 디코더에 의해 복원된다. 이 부분에 해당되는 Loss Function은 다음과 같다.   \\[L_{AE_{1}} = ||W - AE_{1}(W)||_2\\]  \\[L_{AE_{2}} = ||W - AE_{2}(W)||_2\\]  두 번째 단계에서는 $AE_{2}$가 실제 데이터와 $AE_{1}(W)$를 잘 구분할 수 있도록 학습한다. 반면 $AE_{1}$는 $AE_{2}$의 성능을 저하 시키도록 학습된다. 즉 $AE_{1}$(GAN의 Generator)는 W와 $AE_{2}$의 차이를 최소화 하는 것이고 $AE_{2}$ (GAN의 Discriminator)는 이 차이를 최대화 하는 것이다.   \\[L_{AE_{1}} = +||W - AE_{2}(AE_{1}(W))||_2\\]  \\[L_{AE_{2}} = -||W - AE_{2}(AE_{1}(W))||_2\\]  최종 Loss Function은 아래와 같다.     USAD Tensorflow Implementation   데이터:     https://drive.google.com/open?id=1rVJ5ry5GG-ZZi5yI4x9lICB8VhErXwCw   https://drive.google.com/open?id=1iDYc0OEmidN712fquOBRFjln90SbpaE7   import os os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  import numpy as np import tensorflow as tf import pandas as pd from tensorflow.keras import Model from tensorflow.keras.layers import Layer, Dense from sklearn.metrics import roc_curve,roc_auc_score, accuracy_score, f1_score from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn import preprocessing  CONFIG = {     \"Batch\": 7919,     \"Window\": 12,     \"Hidden\": 100,     \"Epoch\": 100 }  class Encoder(Layer):     def __init__(self, in_size, latent_size):         super(Encoder, self).__init__()         self.linear1 = Dense(int(in_size/2), input_shape=(in_size,), activation = \"relu\")         self.linear2 = Dense(int(in_size/4), input_shape=(int(in_size/2),), activation = \"relu\")         self.linear3 = Dense(latent_size, input_shape=(int(in_size/4),), activation = \"relu\")              def call(self, inputs):         x = self.linear1(inputs)         x = self.linear2(x)         x = self.linear3(x)         return x      class Decoder(Layer):     def __init__(self, latent_size, out_size):         super(Decoder, self).__init__()         self.linear1 = Dense(int(out_size/4), input_shape=(latent_size,), activation = \"relu\")         self.linear2 = Dense(int(out_size/2), input_shape=(int(out_size/4),), activation = \"relu\")         self.linear3 = Dense(out_size, input_shape=(int(out_size/2),), activation = 'sigmoid')      def call(self, inputs):         x = self.linear1(inputs)         x = self.linear2(x)         x = self.linear3(x)         return x  class USAD(Model):     def __init__(self, w_size, z_size):         super(USAD, self).__init__()         self.encoder = Encoder(w_size, z_size)         self.decoder1 = Decoder(z_size, w_size)         self.decoder2 = Decoder(z_size, w_size)              def call(self, inputs, epoch):         z = self.encoder(inputs)         w1 = self.decoder1(z)         w2 = self.decoder2(z)         w3 = self.decoder2(self.encoder(w1))         loss1 = 1/epoch*tf.reduce_mean((inputs-w1)**2)+(1-1/epoch)*tf.reduce_mean((inputs-w3)**2)         loss2 = 1/epoch*tf.reduce_mean((inputs-w2)**2)-(1-1/epoch)*tf.reduce_mean((inputs-w3)**2)         return loss1, loss2   def generate_dataset(batch_size, window_size):     normal = pd.read_csv(\"Manufacturing Dataset/test/SWaT_Dataset_Normal_v1.csv\", low_memory=False)#, nrows=1000)     normal = normal.drop([\"Timestamp\" , \"Normal/Attack\" ] , axis = 1)      # Transform all columns into float64     for i in list(normal):          normal[i]=normal[i].apply(lambda x: str(x).replace(\",\" , \".\"))     normal = normal.astype(float)          min_max_scaler = preprocessing.MinMaxScaler()     x = normal.values     x_scaled = min_max_scaler.fit_transform(x)     normal = pd.DataFrame(x_scaled)      attack = pd.read_csv(\"Manufacturing Dataset/test/SWaT_Dataset_Attack_v0.csv\",sep=\";\", low_memory=False)#, nrows=1000)     labels = [ float(label!= 'Normal' ) for label  in attack[\"Normal/Attack\"].values]     attack = attack.drop([\"Timestamp\" , \"Normal/Attack\" ] , axis = 1)          for i in list(attack):         attack[i]=attack[i].apply(lambda x: str(x).replace(\",\" , \".\"))     attack = attack.astype(float)     x = attack.values     x_scaled = min_max_scaler.fit_transform(x)     attack = pd.DataFrame(x_scaled)          windows_normal=normal.values[np.arange(window_size)[None, :] + np.arange(normal.shape[0]-window_size)[:, None]]     windows_attack=attack.values[np.arange(window_size)[None, :] + np.arange(attack.shape[0]-window_size)[:, None]]      windows_normal_train = windows_normal[:int(np.floor(.8 *  windows_normal.shape[0]))]     windows_normal_val = windows_normal[int(np.floor(.8 *  windows_normal.shape[0])):int(np.floor(windows_normal.shape[0]))]      return windows_normal_train, windows_normal_val, windows_attack, labels  windows_normal_train, windows_normal_val, windows_test, y_test = generate_dataset(CONFIG[\"Batch\"], CONFIG[\"Window\"])  w_size=windows_normal_train.shape[1] * windows_normal_train.shape[2] z_size=windows_normal_train.shape[1] * CONFIG[\"Hidden\"]  train_dataset = tf.data.Dataset.from_tensor_slices(     windows_normal_train.astype(np.float32).reshape([windows_normal_train.shape[0], w_size]) ).batch(CONFIG[\"Batch\"], drop_remainder=False)  val_dataset = tf.data.Dataset.from_tensor_slices(     windows_normal_val.astype(np.float32).reshape([windows_normal_val.shape[0], w_size]) ).batch(CONFIG[\"Batch\"], drop_remainder=False)  test_dataset = tf.data.Dataset.from_tensor_slices(     windows_test.astype(np.float32).reshape([windows_test.shape[0], w_size]) ).batch(CONFIG[\"Batch\"], drop_remainder=False)  windows_labels=[] for i in range(len(y_test) - CONFIG[\"Window\"]):     windows_labels.append(list(np.int_(y_test[i:i+CONFIG[\"Window\"]]))) y_test = [1.0 if (np.sum(window) &gt; 0) else 0 for window in windows_labels ]  model = USAD(w_size, z_size) optimizer1 = tf.optimizers.Adam(learning_rate = 0.001) optimizer2 = tf.optimizers.Adam(learning_rate = 0.001) for epoch in range(1, CONFIG[\"Epoch\"] + 1):     for step, batch in enumerate(train_dataset):                         with tf.GradientTape() as tape1, tf.GradientTape() as tape2:                 loss1,loss2 = model(batch, epoch, training = True)                 grads1 = tape1.gradient(loss1, model.encoder.trainable_variables + model.decoder1.trainable_variables)                 grads2 = tape2.gradient(loss2, model.encoder.trainable_variables + model.decoder2.trainable_variables)                 optimizer1.apply_gradients(zip(grads1, model.encoder.trainable_variables + model.decoder1.trainable_variables))                 optimizer2.apply_gradients(zip(grads2, model.encoder.trainable_variables + model.decoder2.trainable_variables))                      loss1_l, loss2_l = [], []     for step, batch in enumerate(val_dataset):         loss1, loss2 = model(batch, epoch, training = False)         loss1_l.append(loss1)         loss2_l.append(loss2)     loss1_avg = tf.reduce_mean(loss1_l)     loss2_avg = tf.reduce_mean(loss2_l)     print(\"Epoch [%d]\" % (epoch), 'val_loss1:', str(loss1_avg.numpy()) + ', val_loss2:', loss2_avg.numpy())    Reference:     https://github.com/manigalati/usad   https://dl.acm.org/doi/10.1145/3394486.3403392  ","categories": ["Deep Learning"],
        "tags": ["Anomaly Detection"],
        "url": "/deeplearning/usad",
        "teaser": null
      },{
        "title": "[YOLOv3] An Incremental Improvement",
        "excerpt":"Introduction   2015년에 바운딩박스와 클래스를 회귀 방식으로 객체 탐지하는 YOLO(You Only Look Once) 모델이 제안됐다. 3년이 지난 후 UW의 Joseph Redmon과 Ali Farhardi는 YOLO를 업그레이드해서 YOLOv3를 다시 제안했다. YOLOv3는 end-to-end로 학습되는 single-stage 모델로 입력 이미지에서 다양한 객체들을 빠르게 검출할 수 있는 능력으로 유명하다. 또한 YOLOv3에서는 Darknet-53이라는 새로운 백본 네트워크가 도입되어 이전 버전보다 더 높은 정확도를 제고한다. 이 알고리즘은 입력 이미지를 그리드 셀로 나누고, 각 셀이 특정 개수의 bounding box를 예측하는 방식을 사용된다. 그리고 예측된 bounding box는 해당 객체의 클래스와 confidence score를 가진다.   Architecture        YOLO v3는 서로 다른 3개의 scale을 사용하여 최종 결과를 예측한다. 먼저 416x416 크기의 이미지를 모델에 넣어서 크기가 52x52, 26x26, 13x13이 되는 레이어에서 특징맵을 추출한다. 그 다음 가장 높은 단계에서 추출된 특징맵을 1x1, 3x3 Conv 레이어로 구성된 네트워크에 넣어주고, Filter의 개수가 512가 되는 지점에서 2배로 upsampling 한 뒤, 바로 아래 단계에서 추출된 특징맵과 Concatenate해준다. 이 과정을 가장 밑 단계에서 추출된 특징맵에서도 적용한다. 이를 통해 총 3개의 Scale를 가진 새로운 특징맵을 얻을 수 있다. 그리고 이 때 각 Scale의 특징맵의 채널 개수가 (255 = 3 x (4 + 1 + Num of Classes))가 되도록 만들어줘야 한다.   이 방법을 통해 더 높은 단계에서의 특징맵으로 부터 fine-grained 정보를 얻고, 더 낮은 단계에서의 특징맵으로부터 더 유용한 semantic 정보를 얻을 수 있다.   Feature Extractor     YOLO v3에서는 Darknet-53을 backbone 으로 사용한다. Darkenet-53은 ResNet-101보다 1.5배 빠르고, ResNet-152와 비슷한 성능을 보이지만 2배 이상 빠르다.   Training Train YOLOv3  앞서 얻은 multi-scale feature maps를 loss function을 통해 학습시킨다.      bounding box offset의 MSE(Mean Squared Error)   객체를 예측하도록 할당된(responsible for) bounding box의 objectness score의 BCE(Binary Cross Entropy)   객체를 예측하도록 할당되지 않은 bounding box의 no objectness score의 BCE   bounding box의 multi-class BCE   그리고 Inference 시에는 마지막 예측 결과에 NMS(Non Maximum Suppression)을 적용한다.   Results     Reference:     https://arxiv.org/pdf/1804.02767.pdf   https://d33wubrfki0l68.cloudfront.net/c6fd049f28b66dbd35faed6965905ec6281f7d7d/c0399/assets/images/yolo/yolo-architecture.webp   https://towardsdatascience.com/dive-really-deep-into-yolo-v3-a-beginners-guide-9e3d2666280e    ","categories": ["Deep Learning"],
        "tags": ["Computer Vision"],
        "url": "/deeplearning/yolov3",
        "teaser": null
      },{
        "title": "[Stable Diffusion] High-Resolution Image Synthesis with Latent Diffusion Models",
        "excerpt":" ","categories": ["Deep Learning"],
        "tags": ["Computer Vision"],
        "url": "/deeplearning/stablediffusion",
        "teaser": null
      }]
